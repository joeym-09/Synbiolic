{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "retrosynthesis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LU0lVqg26Qw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760d3200-efe8-4f13-f59a-a04ba1842d2f"
      },
      "source": [
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.1.0 python=3.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-099745f6a155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chmod +x Miniconda3-latest-Linux-x86_64.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.1.0 python=3.6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: CustomError: Timed out waiting for output iframe load."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJOSch-D2kb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf25fdd5-ecfa-4b71-a6ac-839d3cc3d94e"
      },
      "source": [
        "import os\n",
        "from pymongo import MongoClient\n",
        "import rdkit.Chem as Chem\n",
        "import cPickle as pickle \n",
        "import sys\n",
        "\n",
        "'''\n",
        "For a specific template set, look through all of the precedent reactions and\n",
        "pull the products. This is so we can learn a mapping from product molecules to\n",
        "likely templates\n",
        "'''\n",
        "\n",
        "limit = 1e9\n",
        "\n",
        "db_client = MongoClient('mongodb://username:password@server/authenticationdb', 27017)\n",
        "reaction_db = db_client['reaxys_v2']['reactions']\n",
        "\n",
        "RETRO_TRANSFORMS_CHIRAL = {\n",
        "    'database': 'reaxys_v2',\n",
        "    'collection': 'transforms_retro_v9',\n",
        "    'mincount': 10,\n",
        "    'mincount_chiral': 5\n",
        "}\n",
        "\n",
        "project_root = os.path.dirname(os.path.dirname(__file__))\n",
        "template_name = '{}_{}_{}_{}'.format(\n",
        "    RETRO_TRANSFORMS_CHIRAL['database'],\n",
        "    RETRO_TRANSFORMS_CHIRAL['collection'],\n",
        "    RETRO_TRANSFORMS_CHIRAL['mincount'],\n",
        "    RETRO_TRANSFORMS_CHIRAL['mincount_chiral'],\n",
        ")\n",
        "\n",
        "\n",
        "# Get templates and their refs\n",
        "database = db_client[RETRO_TRANSFORMS_CHIRAL['database']]\n",
        "RETRO_DB = database[RETRO_TRANSFORMS_CHIRAL['collection']]\n",
        "import makeit.retrosynthetic.transformer as transformer \n",
        "RetroTransformerChiral = transformer.RetroTransformer(\n",
        "    mincount=RETRO_TRANSFORMS_CHIRAL['mincount'],\n",
        "    mincount_chiral=RETRO_TRANSFORMS_CHIRAL['mincount_chiral'],\n",
        ")\n",
        "RetroTransformerChiral.load(chiral=True, refs=True, rxns=False) \n",
        "RetroTransformerChiral.reorder()\n",
        "RETRO_CHIRAL_FOOTNOTE = 'Using {} chiral retrosynthesis templates (mincount {} if achiral, mincount {} if chiral) from {}/{}'.format(len(RetroTransformerChiral.templates),\n",
        "    RETRO_TRANSFORMS_CHIRAL['mincount'], \n",
        "    RETRO_TRANSFORMS_CHIRAL['mincount_chiral'], \n",
        "    RETRO_TRANSFORMS_CHIRAL['database'], \n",
        "    RETRO_TRANSFORMS_CHIRAL['collection'])\n",
        "\n",
        "print('Loaded {} templates'.format(len(RetroTransformerChiral.templates)))\n",
        "\n",
        "## Create map from reaction ID to template ID\n",
        "reaction_id_to_template_num = {}\n",
        "template_num_to_template_id = {}\n",
        "for tmp_num, template in enumerate(RetroTransformerChiral.templates):\n",
        "    template_num_to_template_id[tmp_num] = template['_id']\n",
        "    for ref in template['references']:\n",
        "        rxn_id = int(ref.split('-')[0])\n",
        "        reaction_id_to_template_num[rxn_id] = tmp_num\n",
        "\n",
        "print('{} total reaction refs'.format(len(reaction_id_to_template_num)))\n",
        "\n",
        "## Look through reactions now\n",
        "with open(os.path.join(project_root, 'data', 'reaxys_limit%i_%s.txt' % (limit, template_name)), 'w') as f:\n",
        "    i = 0\n",
        "    for rx_doc in reaction_db.find({'RXN_SMILES': {'$exists': True}}, ['_id', 'RXN_SMILES']).sort('_id', 1):\n",
        "        try:\n",
        "            # Only look at reactions that made the template cut\n",
        "            if rx_doc['_id'] not in reaction_id_to_template_num:\n",
        "                continue\n",
        "\n",
        "            r, p = rx_doc['RXN_SMILES'].split('>>')\n",
        "            if (not r) or (not p) or ('.' in p):\n",
        "                continue\n",
        "            r_mol = Chem.MolFromSmiles(str(r))\n",
        "            p_mol = Chem.MolFromSmiles(str(p))\n",
        "            if (not r_mol) or (not p_mol): \n",
        "                continue\n",
        "            [a.ClearProp('molAtomMapNumber') for a in r_mol.GetAtoms() if a.HasProp('molAtomMapNumber')]\n",
        "            [a.ClearProp('molAtomMapNumber') for a in p_mol.GetAtoms() if a.HasProp('molAtomMapNumber')]\n",
        "            n = max(r_mol.GetNumAtoms(), p_mol.GetNumAtoms())\n",
        "            f.write('%s>>%s %i %i %i %s\\n' % (Chem.MolToSmiles(r_mol,True), \n",
        "                Chem.MolToSmiles(p_mol,True), n, rx_doc['_id'], \n",
        "                reaction_id_to_template_num[rx_doc['_id']], \n",
        "                template_num_to_template_id[reaction_id_to_template_num[rx_doc['_id']]]))\n",
        "            i += 1\n",
        "            if i % 1000 == 0:\n",
        "                print('Wrote %i' % i)\n",
        "            if i >= limit:\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-27 00:57:21--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71785000 (68M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "\r          Miniconda   0%[                    ]       0  --.-KB/s               \r         Miniconda3  23%[===>                ]  16.01M  79.4MB/s               \r        Miniconda3-  48%[========>           ]  33.01M  81.8MB/s               \r       Miniconda3-l  77%[==============>     ]  53.01M  87.5MB/s               \rMiniconda3-latest-L 100%[===================>]  68.46M  90.4MB/s    in 0.8s    \n",
            "\n",
            "2019-12-27 00:57:21 (90.4 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [71785000/71785000]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.2.0=py37_0\n",
            "    - ca-certificates==2019.10.16=0\n",
            "    - certifi==2019.9.11=py37_0\n",
            "    - cffi==1.13.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.7.12=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.1=he6710b0_1\n",
            "    - openssl==1.1.1d=h7b6447c_3\n",
            "    - pip==19.3.1=py37_0\n",
            "    - pycosat==0.6.3=py37h14c3975_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.0.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.4=h265db76_1\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_0\n",
            "    - ruamel_yaml==0.15.46=py37h14c3975_0\n",
            "    - setuptools==41.4.0=py37_0\n",
            "    - six==1.12.0=py37_0\n",
            "    - sqlite==3.30.0=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.36.1=py_0\n",
            "    - urllib3==1.24.2=py37_0\n",
            "    - wheel==0.33.6=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.2.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2019.10.16-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.9.11-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.13.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.7.12-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.1-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_3\n",
            "  pip                pkgs/main/linux-64::pip-19.3.1-py37_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h14c3975_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.0.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.4-h265db76_1\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.46-py37h14c3975_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-41.4.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.12.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.30.0-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.36.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.24.2-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.33.6-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m23.173s\n",
            "user\t0m7.157s\n",
            "sys\t0m2.631s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d368df8ba55d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chmod +x Miniconda3-latest-Linux-x86_64.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.1.0 python=3.6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: CustomError: Timed out waiting for output iframe load."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHVj8peo1dM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from utils.nn import linearND\n",
        "import math, sys, random, os\n",
        "from optparse import OptionParser\n",
        "import threading\n",
        "from multiprocessing import Queue, Process\n",
        "import numpy as np\n",
        "from Queue import Empty\n",
        "import time\n",
        "import h5py\n",
        "from itertools import chain\n",
        "import os \n",
        "import cPickle as pickle\n",
        "project_root = os.path.dirname(os.path.dirname(__file__))\n",
        "\n",
        "NK = 100\n",
        "NK0 = 10\n",
        "report_interval = 10\n",
        "min_iterations = 1000\n",
        "\n",
        "\n",
        "parser = OptionParser()\n",
        "parser.add_option(\"-t\", \"--train\", dest=\"train_path\", default=os.path.join(project_root, 'data', 'reaxys_limit10.txt'))\n",
        "parser.add_option(\"-m\", \"--save_dir\", dest=\"save_path\", default=os.path.join(project_root, 'models', 'example_model'))\n",
        "parser.add_option(\"-f\", \"--fp_suffix\", dest=\"fp_suffix\", default=\"_fp.pkl\")\n",
        "parser.add_option(\"--fp_length\", dest=\"fp_length\", default=1024)\n",
        "parser.add_option(\"-b\", \"--batch\", dest=\"batch_size\", default=1024)\n",
        "parser.add_option(\"-w\", \"--hidden\", dest=\"hidden_size\", default=300)\n",
        "parser.add_option(\"-o\", \"--out\", dest=\"output_size\", default=61142)\n",
        "parser.add_option(\"-d\", \"--depth\", dest=\"depth\", default=5)\n",
        "parser.add_option(\"-l\", \"--max_norm\", dest=\"max_norm\", default=5.0)\n",
        "parser.add_option(\"-u\", \"--device\", dest=\"device\", default=\"\")\n",
        "parser.add_option(\"--test\", dest=\"test\", default='')\n",
        "parser.add_option(\"-v\", \"--verbose\", dest=\"verbose_test\", default=False)\n",
        "parser.add_option(\"-c\", \"--checkpoint\", dest=\"checkpoint\", default=\"final\")\n",
        "parser.add_option(\"-s\", \"--saveint\", dest=\"save_interval\", default=0)\n",
        "parser.add_option(\"-i\", \"--interactive\", dest=\"interactive\", default=False)\n",
        "parser.add_option(\"--fp_len\", dest=\"fp_len\", default=1024)\n",
        "parser.add_option(\"--fp_rad\", dest=\"fp_rad\", default=2)\n",
        "parser.add_option(\"--fixed_epochs_train_all\", dest=\"fixed_epochs_train_all\", default=0)\n",
        "opts,args = parser.parse_args()\n",
        "\n",
        "batch_size = int(opts.batch_size)\n",
        "hidden_size = int(opts.hidden_size)\n",
        "depth = int(opts.depth)\n",
        "max_norm = float(opts.max_norm)\n",
        "test = opts.test\n",
        "save_interval = int(opts.save_interval)\n",
        "verbose_test = bool(opts.verbose_test)\n",
        "interactive_mode = bool(opts.interactive)\n",
        "output_size = int(opts.output_size)\n",
        "fixed_epochs_train_all = int(opts.fixed_epochs_train_all)\n",
        "\n",
        "max_save = 20 if not fixed_epochs_train_all else fixed_epochs_train_all\n",
        "\n",
        "FP_len = int(opts.fp_len)\n",
        "FP_rad = int(opts.fp_rad)\n",
        "\n",
        "\n",
        "if interactive_mode:\n",
        "    batch_size = 2 # keep it small\n",
        "\n",
        "if not os.path.isdir(opts.save_path):\n",
        "    os.mkdir(opts.save_path)\n",
        "\n",
        "import rdkit.Chem.AllChem as AllChem\n",
        "def mol_to_fp(mol, radius=FP_rad, nBits=FP_len):\n",
        "    if mol is None:\n",
        "        return np.zeros((nBits,), dtype=np.float32)\n",
        "    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits, \n",
        "        useChirality=True), dtype=np.bool)\n",
        "\n",
        "def smi_to_fp(smi, radius=FP_rad, nBits=FP_len):\n",
        "    if not smi:\n",
        "        return np.zeros((nBits,), dtype=np.float32)\n",
        "    return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits)\n",
        "\n",
        "gpu_options = tf.GPUOptions(allow_growth=True, visible_device_list=opts.device)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as session:\n",
        "    _input_mol = tf.placeholder(tf.float32, [batch_size, FP_len])\n",
        "    _label = tf.placeholder(tf.int32, [batch_size,])\n",
        "\n",
        "    q = tf.FIFOQueue(20, [tf.float32, tf.int32]) # fixed size\n",
        "    enqueue = q.enqueue([_input_mol, _label])\n",
        "    [input_mol, label] = q.dequeue()\n",
        "    src_holder = [input_mol, label]\n",
        "\n",
        "    input_mol.set_shape([batch_size, FP_len])\n",
        "    label.set_shape([batch_size,])\n",
        "    mol_hiddens = tf.nn.relu(linearND(input_mol, hidden_size, scope=\"encoder0\"))\n",
        "    for d in xrange(1, depth):\n",
        "        mol_hiddens = tf.nn.relu(linearND(mol_hiddens, hidden_size, scope=\"encoder%i\"%d))\n",
        "\n",
        "    score = linearND(mol_hiddens, output_size, scope=\"output\")\n",
        "    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=score, labels=label))\n",
        "    _, topk = tf.nn.top_k(score, k=NK)\n",
        "\n",
        "    # For normal reaction-wise training\n",
        "    _lr = tf.placeholder(tf.float32, [])\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=_lr)\n",
        "    param_norm = tf.global_norm(tf.trainable_variables())\n",
        "    grads_and_vars = optimizer.compute_gradients(loss / batch_size)\n",
        "    grads, var = zip(*grads_and_vars)\n",
        "    grad_norm = tf.global_norm(grads)\n",
        "    new_grads, _ = tf.clip_by_global_norm(grads, max_norm)\n",
        "    grads_and_vars = zip(new_grads, var)\n",
        "    backprop = optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "\n",
        "    tf.global_variables_initializer().run(session=session)\n",
        "    size_func = lambda v: reduce(lambda x, y: x*y, v.get_shape().as_list())\n",
        "    n = sum(size_func(v) for v in tf.trainable_variables())\n",
        "    print \"Model size: %dK\" % (n/1000,)\n",
        "\n",
        "    queue = Queue()\n",
        "\n",
        "\n",
        "    def read_data_once(path, coord, frag='valid'):\n",
        "        print('Loading data file')\n",
        "        with open(path + '.data_pkl', 'r') as f:\n",
        "            data = pickle.load(f)\n",
        "        print('Loading fingerprint file')\n",
        "        with open(path + '.fp_pkl', 'r') as f:\n",
        "            FPs = pickle.load(f)\n",
        "\n",
        "        data_len = len(data)\n",
        "        print('%i total data entries' % data_len)\n",
        "        if frag == 'train':\n",
        "            data = data[:int(0.8 * data_len)]\n",
        "            FPs = FPs[:int(0.8 * data_len), :]\n",
        "            data_len = len(data)\n",
        "            print('Taking 0.8 as training set (%i)' % data_len)\n",
        "        elif frag == 'valid':\n",
        "            data = data[int(0.8 * data_len):int(0.9 * data_len)]\n",
        "            FPs = FPs[int(0.8 * data_len):int(0.9 * data_len), :]\n",
        "            data_len = len(data)\n",
        "            print('Taking 0.1 as validation set (%i)' % data_len)\n",
        "        elif frag == 'test':\n",
        "            data = data[int(0.9 * data_len):]\n",
        "            FPs = FPs[int(0.9 * data_len):, :]\n",
        "            data_len = len(data)\n",
        "            print('Taking 0.1 as test set (%i)' % data_len)\n",
        "        else:\n",
        "            raise ValueError('Unknown data frag type')\n",
        "        it = 0\n",
        "        src_mols = np.zeros((batch_size, FP_len), dtype=np.float32)\n",
        "        src_labels = np.array([0 for i in range(batch_size)], dtype=np.int32)\n",
        "        while it < data_len:\n",
        "\n",
        "            # Try to get all FPs in one read (faster)\n",
        "            if (it + batch_size) <= data_len:\n",
        "                src_mols = FPs[it:it+batch_size, :].todense().astype(np.float32)\n",
        "                src_labels = np.array([ex[4] for ex in data[it:it+batch_size]], dtype=np.int32) # template num\n",
        "                src_info = data[it:it+batch_size]\n",
        "                it = it + batch_size\n",
        "            # If we are at the end, do one-by-one)\n",
        "            else:\n",
        "                src_info = []\n",
        "                for i in xrange(batch_size):\n",
        "                    if it >= data_len:\n",
        "                        src_mols[i,:] = 0. # 0 out fingerprint\n",
        "                        src_labels[i] = 0. # doesn't matter\n",
        "                        src_info.append([])\n",
        "                    else:\n",
        "                        src_mols[i,:] = FPs[it,:].todense().astype(np.float32)\n",
        "                        src_info.append(data[it])\n",
        "                        src_labels[i] = data[it][4] # template_num\n",
        "                    it = it + 1\n",
        "\n",
        "            session.run(enqueue, feed_dict={_input_mol: src_mols, _label: src_labels})\n",
        "            queue.put(src_info)\n",
        "            # print('Queue size: {}'.format(queue.qsize()))\n",
        "            # sys.stdout.flush()\n",
        "\n",
        "        # Stop signal for testing\n",
        "        queue.put(None)\n",
        "        coord.request_stop()\n",
        "\n",
        "    def read_data_master(path, coord):\n",
        "        with open(path + '.data_pkl', 'r') as f:\n",
        "            print('loading data')\n",
        "            data = pickle.load(f)\n",
        "                \n",
        "        with open(path + '.fp_pkl', 'r') as f:\n",
        "            print('loading sparse FP file')\n",
        "            FPs = pickle.load(f)\n",
        "\n",
        "        data_len = len(data)\n",
        "        print('%i total data entries' % data_len)\n",
        "\n",
        "        if not fixed_epochs_train_all:\n",
        "            print('...slicing data')\n",
        "            data = data[:int(0.8 * data_len)]  \n",
        "            FPs = FPs[:int(0.8 * data_len), :]\n",
        "            data_len = len(data)\n",
        "            print('Taking 0.8 for training (%i)' % data_len)\n",
        "        else:\n",
        "            print('Using WHOLE DATA for fixed number of epochs')\n",
        "\n",
        "        \n",
        "        it = 0; \n",
        "        src_mols = np.zeros((batch_size, FP_len), dtype=np.float32)\n",
        "        src_labels = np.array([0 for i in range(batch_size)], dtype=np.int32)\n",
        "        while not coord.should_stop():\n",
        "\n",
        "            # Try to get all FPs in one read (faster)\n",
        "            # Try to get all FPs in one read (faster)\n",
        "            if (it + batch_size) <= data_len:\n",
        "                src_mols = FPs[it:it+batch_size, :].todense().astype(np.float32)\n",
        "                src_labels = np.array([ex[4] for ex in data[it:it+batch_size]], dtype=np.int32) # template num\n",
        "                src_info = data[it:it+batch_size]\n",
        "                it = (it + batch_size) % data_len\n",
        "\n",
        "            # If we are at the end (where we need to loop around, do one-by-one)\n",
        "            else:\n",
        "                src_info = []\n",
        "\n",
        "                for i in xrange(batch_size):\n",
        "                    src_mols[i,:] = FPs[it,:].todense().astype(np.float32)\n",
        "                    src_info.append(data[it])\n",
        "                    src_labels[i] = data[it][4] # template_num\n",
        "                    it = (it + 1) % data_len\n",
        "            # print(src_mols)\n",
        "            # print(src_labels)\n",
        "            session.run(enqueue, feed_dict={_input_mol: src_mols, _label: src_labels})\n",
        "            queue.put(src_info)\n",
        "            #print('Queue size: {}'.format(queue.qsize()))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        coord.request_stop()\n",
        "        f.close()\n",
        "\n",
        "    def dummy_thread():\n",
        "        return\n",
        "\n",
        "    coord = tf.train.Coordinator()\n",
        "    if interactive_mode:\n",
        "        all_threads = [threading.Thread(target=dummy_thread)]\n",
        "    elif test:\n",
        "        all_threads = [threading.Thread(target=read_data_once, args=(opts.train_path, coord), kwargs={'frag': opts.test})]\n",
        "        print('Added read_data_once')\n",
        "    else:\n",
        "        all_threads = [threading.Thread(target=read_data_master, args=(opts.train_path, coord))]\n",
        "        print('Added read_data_master')\n",
        "\n",
        "    [t.start() for t in all_threads]\n",
        "\n",
        "    if not interactive_mode:\n",
        "        print('Reading data file to figure out data length')\n",
        "\n",
        "        with open(opts.train_path + '.data_pkl', 'r') as f:\n",
        "            data_len = len(pickle.load(f))\n",
        "\n",
        "        print('Data length: %i' % data_len)\n",
        "        if save_interval == 0: # approx once per epoch\n",
        "            save_interval = np.ceil(data_len / float(batch_size))\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=None)\n",
        "    if test or interactive_mode:\n",
        "        if opts.checkpoint:\n",
        "            restore_path = os.path.join(opts.save_path, 'model.%s' % opts.checkpoint)\n",
        "        else:\n",
        "            restore_path = tf.train.latest_checkpoint(opts.save_path)\n",
        "        saver.restore(session, restore_path)\n",
        "        print('Restored values from latest saved file ({})'.format(restore_path))\n",
        "        test_path = '%s.prediced.%s.%s' % (restore_path, os.path.basename(opts.train_path), str(opts.test))\n",
        "        summary_path = os.path.join(opts.save_path, 'model.%s.summary' % os.path.basename(opts.train_path))\n",
        "    it, sum_diff, sum_gnorm, = 0, 0.0, 0.0\n",
        "    sum_loss = 0.0;\n",
        "    sum_acc1 = 0.0;\n",
        "    sum_acc5 = 0.0;\n",
        "    sum_acc10 = 0.0;\n",
        "    sum_acc20 = 0.0;\n",
        "    sum_acc50 = 0.0;\n",
        "    sum_acc100 = 0.0;\n",
        "\n",
        "    lr = 0.001\n",
        "    try:\n",
        "        if interactive_mode:\n",
        "            pass\n",
        "            # prompt = raw_input('enter a tag for this session: ')\n",
        "            # interactive_path = '%s.interactive.%s' % (restore_path, prompt.strip())\n",
        "            # fid = open(interactive_path, 'a')\n",
        "\n",
        "            # def get_score_from_smi(smi):\n",
        "            #     if not smi:\n",
        "            #         return ('', 0.)\n",
        "            #     src_batch = [smi]\n",
        "            #     while len(src_batch) != (batch_size * 2): # round out last batch\n",
        "            #         src_batch.append('')\n",
        "            #     src_mols = np.array(map(smi_to_fp, src_batch), dtype=np.float32)\n",
        "            #     if sum(sum(src_mols)) == 0:\n",
        "            #         print('Could not get fingerprint?')\n",
        "            #         cur_score = [0.]\n",
        "            #     else:\n",
        "            #         # Run\n",
        "            #         cur_score, = session.run([score], feed_dict={\n",
        "            #             input_mol: src_mols,\n",
        "            #             _lr: 0.001,\n",
        "            #         })\n",
        "            #         print('Score: {}'.format(cur_score[0]))\n",
        "            #     mol = Chem.MolFromSmiles(smi)\n",
        "            #     if mol:\n",
        "            #         smi = Chem.MolToSmiles(mol, isomericSmiles=True, kekuleSmiles=True)\n",
        "            #     else:\n",
        "            #         smi = ''\n",
        "            #     return (smi, cur_score[0])\n",
        "\n",
        "            # while True:\n",
        "            #     try:\n",
        "            #         prompt = raw_input('\\nEnter SMILES (or quit): ')\n",
        "            #         if prompt.strip() == 'quit':\n",
        "            #             break\n",
        "            #         if str('>>') in prompt: # reaction\n",
        "            #             reactants = prompt.strip().split('>>')[0].split('.')\n",
        "            #             reactants_smi = []\n",
        "            #             reactants_score = 0.\n",
        "            #             for reactant in reactants:\n",
        "            #                 (smi, cur_score) = get_score_from_smi(reactant)\n",
        "            #                 reactants_smi.append(smi)\n",
        "            #                 reactants_score = max(reactants_score, cur_score)\n",
        "            #             products = prompt.strip().split('>>')[1].split('.')\n",
        "            #             products_smi = []\n",
        "            #             products_score = 0.\n",
        "            #             for product in products:\n",
        "            #                 (smi, cur_score) = get_score_from_smi(product)\n",
        "            #                 products_smi.append(smi)\n",
        "            #                 products_score = max(products_score, cur_score)\n",
        "            #             smi = '{}>>{}'.format('.'.join(reactants_smi), '.'.join(products_smi))\n",
        "            #             fid.write('%s %s %.4f %.4f %.4f\\n' % (prompt.strip(), smi, reactants_score, products_score, products_score-reactants_score))\n",
        "            #         else: # single or list of mols\n",
        "            #             reactants = prompt.strip().split('.')\n",
        "            #             reactants_smi = []\n",
        "            #             reactants_score = 0.\n",
        "            #             for reactant in reactants:\n",
        "            #                 (smi, cur_score) = get_score_from_smi(reactant)\n",
        "            #                 reactants_smi.append(smi)\n",
        "            #                 reactants_score = max(reactants_score, cur_score)\n",
        "            #             fid.write('%s %s %.4f\\n' % (prompt.strip(), '.'.join(reactants_smi), reactants_score))\n",
        "\n",
        "            #     except KeyboardInterrupt:\n",
        "            #         print('Breaking out of prompt')\n",
        "            #         fid.close()\n",
        "            #         raise KeyboardInterrupt\n",
        "            #     except Exception as e:\n",
        "            #         print(e)\n",
        "            #         fid.write('%s\\n' % prompt.strip())\n",
        "            #         continue\n",
        "        elif test:\n",
        "            while queue.qsize() == 0:\n",
        "                print('Letting queue fill up (10 s...)')\n",
        "                time.sleep(10)\n",
        "\n",
        "\n",
        "            summarystring = ''\n",
        "            ctr = 0.0\n",
        "            if verbose_test: \n",
        "                learned_scores = []\n",
        "\n",
        "            sum_diff_is_pos = 0.0\n",
        "            sum_diff_is_big = 0.0\n",
        "            sum_diff = 0.0\n",
        "            sum_gnorm = 0.0\n",
        "            sum_loss = 0.0\n",
        "            while True:\n",
        "                try:\n",
        "                    (src_info) = queue.get(timeout=600)\n",
        "                    if src_info is None:\n",
        "                        raise Empty\n",
        "                    cur_topk, cur_score, pnorm, gnorm, cur_loss = session.run([topk, score, param_norm, grad_norm, loss], feed_dict={_lr:lr})\n",
        "                    \n",
        "                    it += 1\n",
        "\n",
        "                    # Padded\n",
        "                    src_info = [_ for _ in src_info if _]\n",
        "                    ctr += len(src_info)\n",
        "                    if len(src_info) < batch_size:\n",
        "                        print('Found an incomplete batch with only length...')\n",
        "                        print(len(src_info))\n",
        "\n",
        "                    # print(src_info[0][4])\n",
        "                    # print(list(cur_topk[0,:10]))\n",
        "                    # print('')\n",
        "\n",
        "                    sum_acc1 += sum([int(src_info[i][4]) in list(cur_topk[i,:1]) for i in range(len(src_info))])\n",
        "                    sum_acc5 += sum([int(src_info[i][4]) in list(cur_topk[i,:5]) for i in range(len(src_info))])\n",
        "                    sum_acc10 += sum([int(src_info[i][4]) in list(cur_topk[i,:10]) for i in range(len(src_info))])\n",
        "                    sum_acc20 += sum([int(src_info[i][4]) in list(cur_topk[i,:20]) for i in range(len(src_info))])\n",
        "                    sum_acc50 += sum([int(src_info[i][4]) in list(cur_topk[i,:50]) for i in range(len(src_info))])\n",
        "                    sum_acc100 += sum([int(src_info[i][4]) in list(cur_topk[i,:100]) for i in range(len(src_info))])\n",
        "                    sum_gnorm += gnorm\n",
        "                    sum_loss += cur_loss\n",
        "                        \n",
        "                    # if verbose_test:\n",
        "                    #     for i in range(len(ids_batch)):\n",
        "                    #         learned_scores.append(cur_score[2*i])\n",
        "                    #         learned_scores.append(cur_score[i*2+1])\n",
        "\n",
        "                    if it % report_interval == 0:\n",
        "                        summarystring = \"[%09i prods seen], Acc1: %.3f, Acc5: %.3f, Acc10: %.3f, Acc20: %.3f, Acc50: %.3f, Acc100: %.3f, PNorm: %.2f, GNorm: %.2f, Loss: %.4f\" % \\\n",
        "                            (ctr, sum_acc1 / float(ctr), \n",
        "                            sum_acc5 / float(ctr),\n",
        "                            sum_acc10 / float(ctr),\n",
        "                            sum_acc20 / float(ctr),\n",
        "                            sum_acc50 / float(ctr),\n",
        "                            sum_acc100 / float(ctr),\n",
        "                            pnorm, sum_gnorm / float(ctr),\n",
        "                            sum_loss / float(ctr)) \n",
        "\n",
        "                        print(summarystring)\n",
        "                        sys.stdout.flush()\n",
        "\n",
        "                except Empty:\n",
        "                    print('End of data queue I think...have seen {} examples'.format(ctr))\n",
        "                    break\n",
        "\n",
        "            summarystring = \"[%09i prods seen], Acc1: %.3f, Acc5: %.3f, Acc10: %.3f, Acc20: %.3f, Acc50: %.3f, Acc100: %.3f, PNorm: %.2f, GNorm: %.2f, Loss: %.4f\" % \\\n",
        "                (ctr, sum_acc1 / float(ctr), \n",
        "                sum_acc5 / float(ctr),\n",
        "                sum_acc10 / float(ctr),\n",
        "                sum_acc20 / float(ctr),\n",
        "                sum_acc50 / float(ctr),\n",
        "                sum_acc100 / float(ctr),\n",
        "                pnorm, sum_gnorm / float(ctr),\n",
        "                sum_loss / float(ctr)) \n",
        "\n",
        "            print(summarystring)\n",
        "            sys.stdout.flush()\n",
        "            fidsum = open(summary_path, 'a')\n",
        "            fidsum.write('[%s = %s] %s\\n' % (opts.checkpoint, opts.test, summarystring))\n",
        "            fidsum.close()\n",
        "\n",
        "            # if verbose_test: \n",
        "            #     fid = h5py.File(test_path + '.h5', 'w')\n",
        "            #     dset = fid.create_dataset('learned_scores', (len(learned_scores),), dtype=np.float32)\n",
        "            #     dset[:] = np.array(learned_scores)\n",
        "            #     fid.close()\n",
        "        else:\n",
        "            hist_fid = open(opts.save_path + \"/model.hist\", \"a\")\n",
        "\n",
        "            print('Letting queue fill up (10 s)')\n",
        "            time.sleep(10)\n",
        "            \n",
        "            while not coord.should_stop():               \n",
        "                it += 1\n",
        "                _, cur_topk, cur_score, pnorm, gnorm, cur_loss = session.run([backprop, topk, score, param_norm, grad_norm, loss], feed_dict={_lr:lr})\n",
        "                (src_info) = queue.get()\n",
        "                \n",
        "                # print(src_info[0][4])\n",
        "                # print(list(cur_topk[0, :5]))\n",
        "                sum_acc1 += sum([int(src_info[i][4]) in list(cur_topk[i,:1]) for i in range(len(src_info))])\n",
        "                sum_acc5 += sum([int(src_info[i][4]) in list(cur_topk[i,:5]) for i in range(len(src_info))])\n",
        "                sum_acc10 += sum([int(src_info[i][4]) in list(cur_topk[i,:10]) for i in range(len(src_info))])\n",
        "                sum_acc20 += sum([int(src_info[i][4]) in list(cur_topk[i,:20]) for i in range(len(src_info))])\n",
        "                sum_acc50 += sum([int(src_info[i][4]) in list(cur_topk[i,:50]) for i in range(len(src_info))])\n",
        "                sum_acc100 += sum([int(src_info[i][4]) in list(cur_topk[i,:100]) for i in range(len(src_info))])\n",
        "                sum_gnorm += gnorm\n",
        "                sum_loss += cur_loss\n",
        "\n",
        "                # print(sum_acc1)\n",
        "                # print(batch_size)\n",
        "                # print(it)\n",
        "                # print(report_interval)\n",
        "                # print(sum_acc5)\n",
        "                # print(sum_acc10)\n",
        "                # print(sum_acc20)\n",
        "                # print(sum_acc50)\n",
        "                # # print(sum_loss)\n",
        "                # print(cur_loss)\n",
        "\n",
        "                # print(type(sum_acc1))\n",
        "                # print(type(batch_size))\n",
        "                # print(type(it))\n",
        "                # print(type(report_interval))\n",
        "                # print(type(sum_acc5))\n",
        "                # print(type(sum_acc10))\n",
        "                # print(type(sum_acc20))\n",
        "                # print(type(sum_acc50))\n",
        "                # print(type(sum_loss))\n",
        "                # print(type(pnorm))\n",
        "                # print(type(sum_gnorm))\n",
        "                 \n",
        "                if it % min(report_interval, save_interval) == 0:\n",
        "                    logstr = \"it %06i [%09i prods seen], Acc1: %.3f, Acc5: %.3f, Acc10: %.3f, Acc20: %.3f, Acc50: %.3f, Acc100: %.3f, PNorm: %.2f, GNorm: %.2f, Loss: %.4f\" % \\\n",
        "                        (it, it*batch_size, sum_acc1 / float(report_interval * batch_size), \n",
        "                            sum_acc5 / float(report_interval * batch_size),\n",
        "                            sum_acc10 / float(report_interval * batch_size),\n",
        "                            sum_acc20 / float(report_interval * batch_size),\n",
        "                            sum_acc50 / float(report_interval * batch_size),\n",
        "                            sum_acc100 / float(report_interval * batch_size),\n",
        "                            pnorm, sum_gnorm / report_interval,\n",
        "                            sum_loss / report_interval) \n",
        "                    hist_fid.write(logstr + \"\\n\")\n",
        "                    print(logstr)\n",
        "                    sys.stdout.flush()\n",
        "                    sum_gnorm = 0.0\n",
        "                    sum_loss = 0.0\n",
        "                    sum_loss = 0.0;\n",
        "                    sum_acc1 = 0.0;\n",
        "                    sum_acc5 = 0.0;\n",
        "                    sum_acc10 = 0.0;\n",
        "                    sum_acc20 = 0.0;\n",
        "                    sum_acc50 = 0.0;\n",
        "                    sum_acc100 = 0.0;\n",
        "\n",
        "                    # print('Ex: {:.2f}>>{:.2f} -> diff = {:.2f}'.format(\n",
        "                    #     cur_score[0], cur_score[1], cur_diff[0]))\n",
        "                    # print('Ex: ID{} === {}>>{}'.format(\n",
        "                    #     ids_batch[0], src_batch[0], src_batch[1]))\n",
        "\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                if it % save_interval == 0:\n",
        "                    lr *= 0.9\n",
        "                    saver.save(session, opts.save_path + \"/model.ckpt\", global_step=it)\n",
        "                    print \"Model Saved! Decaying learning rate\"\n",
        "\n",
        "                if it >= max(min_iterations, max_save * save_interval):\n",
        "                    coord.request_stop()\n",
        "\n",
        "    except Exception as e:\n",
        "        print e\n",
        "        coord.request_stop(e)\n",
        "    finally:\n",
        "        if not test and not interactive_mode: \n",
        "            saver.save(session, opts.save_path + \"/model.final\")\n",
        "            hist_fid.close()\n",
        "        coord.request_stop()\n",
        "        coord.join(all_threads)\n",
        "        try:\n",
        "            [p.join() for p in processes]\n",
        "        except Exception:\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDFIF8nC1rYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import math, sys, random, os\n",
        "import numpy as np\n",
        "import time\n",
        "import rdkit.Chem as Chem \n",
        "import rdkit.Chem.AllChem as AllChem\n",
        "\n",
        "import os \n",
        "project_root = os.path.dirname(os.path.dirname(__file__))\n",
        "\n",
        "FP_len = 2048\n",
        "FP_rad = 2\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "    \n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "class RetroTempPrioritizer():\n",
        "    def __init__(self, FP_len=FP_len):\n",
        "        self.vars = []\n",
        "        self.FP_len = FP_len\n",
        "        self._restored = False\n",
        "\n",
        "    def restore(self, weight_path=os.path.join(project_root, 'models', '6d3M_Reaxys_10_5', 'model.ckpt-92820.as_numpy.pickle')):\n",
        "        import cPickle as pickle\n",
        "        with open(weight_path, 'rb') as fid:\n",
        "            self.vars = pickle.load(fid)\n",
        "        print('Restored variables from {}'.format(weight_path))\n",
        "        self._restored = True\n",
        "        return self\n",
        "\n",
        "    def apply(self, x):\n",
        "        if not self._restored:\n",
        "            raise ValueError('Must restore model weights!')\n",
        "        # Each pair of vars is a weight and bias term\n",
        "        for i in range(0, len(self.vars), 2):\n",
        "            last_layer = (i == len(self.vars)-2)\n",
        "            W = self.vars[i] \n",
        "            b = self.vars[i+1]\n",
        "            x = np.matmul(x, W) + b\n",
        "            if not last_layer:\n",
        "                x = x * (x > 0) # ReLU\n",
        "        return x\n",
        "\n",
        "\n",
        "    def mol_to_fp(self, mol, radius=FP_rad):\n",
        "        if mol is None:\n",
        "            return np.zeros((nBits,), dtype=np.float32)\n",
        "        return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=self.FP_len, \n",
        "            useChirality=True), dtype=np.bool)\n",
        "\n",
        "    def smi_to_fp(self, smi, radius=FP_rad):\n",
        "        if not smi:\n",
        "            return np.zeros((self.FP_len,), dtype=np.float32)\n",
        "        return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits)\n",
        "\n",
        "    def get_topk_from_smi(self, smi='', k=100):\n",
        "        if not smi:\n",
        "            return []\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if not mol:\n",
        "            return []\n",
        "        return self.get_topk_from_mol(mol, k=k)\n",
        "        \n",
        "    def get_topk_from_mol(self, mol, k=100):\n",
        "        fp = self.mol_to_fp(mol).astype(np.float32)\n",
        "        cur_scores = self.apply(fp)\n",
        "        indices = list(cur_scores.argsort()[-k:][::-1])\n",
        "        cur_scores.sort()\n",
        "        probs = softmax(cur_scores)\n",
        "        return probs[-k:][::-1], indices\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = RetroTempPrioritizer(FP_len=2048)    \n",
        "    model.restore(os.path.join(project_root, 'models', '6d3M_Reaxys_10_5', 'model.ckpt-92820.as_numpy.pickle'))\n",
        "\n",
        "    smis = ['CCCOCCC', 'CCCNc1ccccc1']\n",
        "    for smi in smis:\n",
        "        lst = model.get_topk_from_smi(smi)\n",
        "        print('{} -> {}'.format(smi, lst))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}