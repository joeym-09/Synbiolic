{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Synbiolic_vae_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS8DgoDpXQbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install deepchem + rdkit \n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.1.0 python=3.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31KFiVqHXgwM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "376c345e-6ffc-400e-8c8a-29cac7c56b10"
      },
      "source": [
        "import sys\n",
        "if sys.version_info[0] >= 3:\n",
        "    sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " '/usr/local/lib/python3.6/site-packages/',\n",
              " '/usr/local/lib/python3.6/site-packages/',\n",
              " '/usr/local/lib/python3.6/site-packages/',\n",
              " '/usr/local/lib/python3.6/site-packages/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sme2DmMX8Gd",
        "colab_type": "text"
      },
      "source": [
        "### **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZkl1jDWYQYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit import DataStructs\n",
        "from sklearn.model_selection import KFold, StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otird5-PYR47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_fp(smiles):\n",
        "    fp = []\n",
        "    processed_indices = []\n",
        "    invalid_indices = []\n",
        "    for i in range(len(smiles)):\n",
        "        mol = smiles[i]\n",
        "        tmp = np.array(mol2image(mol, n=2048))\n",
        "        if np.isnan(tmp[0]):\n",
        "            invalid_indices.append(i)\n",
        "        else:\n",
        "            fp.append(tmp)\n",
        "            processed_indices.append(i)\n",
        "    return np.array(fp), processed_indices, invalid_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6X5omaFYSCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_desc(smiles, calc):\n",
        "    desc = []\n",
        "    processed_indices = []\n",
        "    invalid_indices = []\n",
        "    for i in range(len(smiles)):\n",
        "        sm = smiles[i]\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(sm)\n",
        "            tmp = np.array(calc(mol))\n",
        "            desc.append(tmp)\n",
        "            processed_indices.append(i)\n",
        "        except:\n",
        "            invalid_indices.append(i)\n",
        "\n",
        "    desc_array = np.array(desc)\n",
        "    return desc_array, processed_indices, invalid_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Beu4w0skYSNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_desc(desc_array, desc_mean=None):\n",
        "    desc_array = np.array(desc_array).reshape(len(desc_array), -1)\n",
        "    ind = np.zeros(desc_array.shape)\n",
        "    for i in range(desc_array.shape[0]):\n",
        "        for j in range(desc_array.shape[1]):\n",
        "            try:\n",
        "                if np.isfinite(desc_array[i, j]):\n",
        "                    ind[i, j] = 1\n",
        "            except:\n",
        "                pass\n",
        "    for i in range(desc_array.shape[0]):\n",
        "        for j in range(desc_array.shape[1]):\n",
        "            if ind[i, j] == 0:\n",
        "                desc_array[i, j] = 0\n",
        "    if desc_mean is None:\n",
        "        desc_mean = np.mean(desc_array, axis=0)\n",
        "    for i in range(desc_array.shape[0]):\n",
        "        for j in range(desc_array.shape[1]):\n",
        "            if ind[i, j] == 0:\n",
        "                desc_array[i, j] = desc_mean[j]\n",
        "    return desc_array, desc_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJIGl9gpYcpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mol2image(x, n=2048):\n",
        "    try:\n",
        "        m = Chem.MolFromSmiles(x)\n",
        "        fp = Chem.RDKFingerprint(m, maxPath=4, fpSize=n)\n",
        "        res = np.zeros(len(fp))\n",
        "        DataStructs.ConvertToNumpyArray(fp, res)\n",
        "        return res\n",
        "    except:\n",
        "        return [np.nan]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPXdevqTYh01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sanitize_smiles(smiles, canonical=True, throw_warning=False):\n",
        "    \"\"\"\n",
        "    Takes list of SMILES strings and returns list of their sanitized versions.\n",
        "    For definition of sanitized SMILES check\n",
        "    http://www.rdkit.org/docs/api/rdkit.Chem.rdmolops-module.html#SanitizeMol\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles: list\n",
        "        list of SMILES strings\n",
        "    canonical: bool (default True)\n",
        "        parameter specifying whether SMILES will be converted to canonical\n",
        "        format\n",
        "    throw_warning: bool (default False)\n",
        "        parameter specifying whether warnings will be thrown if a SMILES is\n",
        "        invalid\n",
        "    Returns\n",
        "    -------\n",
        "    new_smiles: list\n",
        "        list of SMILES and NaNs if SMILES string is invalid or unsanitized.\n",
        "        If canonical is True, returns list of canonical SMILES.\n",
        "    When canonical is True this function is analogous to:\n",
        "        canonical_smiles(smiles, sanitize=True).\n",
        "    \"\"\"\n",
        "    new_smiles = []\n",
        "    for sm in smiles:\n",
        "        try:\n",
        "            if canonical:\n",
        "                new_smiles.append(Chem.MolToSmiles(Chem.MolFromSmiles(sm, sanitize=True)))\n",
        "            else:\n",
        "                new_smiles.append(sm)\n",
        "        except:\n",
        "            if throw_warning:\n",
        "                warnings.warn('Unsanitized SMILES string: ' + sm, UserWarning)\n",
        "            new_smiles.append('')\n",
        "    return new_smiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF3Ke4aTYkrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def canonical_smiles(smiles, sanitize=True, throw_warning=False):\n",
        "    \"\"\"\n",
        "    Takes list of SMILES strings and returns list of their canonical SMILES.\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles: list\n",
        "        list of SMILES strings to convert into canonical format\n",
        "    sanitize: bool (default True)\n",
        "        parameter specifying whether to sanitize SMILES or not.\n",
        "            For definition of sanitized SMILES check\n",
        "            http://www.rdkit.org/docs/api/rdkit.Chem.rdmolops-module.html#SanitizeMol\n",
        "    throw_warning: bool (default False)\n",
        "        parameter specifying whether warnings will be thrown if a SMILES is\n",
        "        invalid\n",
        "    Returns\n",
        "    -------\n",
        "    new_smiles: list\n",
        "        list of canonical SMILES and NaNs if SMILES string is invalid or\n",
        "        unsanitized (when sanitize is True)\n",
        "    When sanitize is True the function is analogous to:\n",
        "        sanitize_smiles(smiles, canonical=True).\n",
        "    \"\"\"\n",
        "    new_smiles = []\n",
        "    for sm in smiles:\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(sm, sanitize=sanitize)\n",
        "            new_smiles.append(Chem.MolToSmiles(mol))\n",
        "        except:\n",
        "            if throw_warning:\n",
        "                warnings.warn(sm + ' can not be canonized: invalid '\n",
        "                                   'SMILES string!', UserWarning)\n",
        "            new_smiles.append('')\n",
        "    return new_smiles\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKEhwZvkYnqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_smi_to_file(filename, smiles, unique=True):\n",
        "    \"\"\"\n",
        "    Takes path to file and list of SMILES strings and writes SMILES to the specified file.\n",
        "        Args:\n",
        "            filename (str): path to the file\n",
        "            smiles (list): list of SMILES strings\n",
        "            unique (bool): parameter specifying whether to write only unique copies or not.\n",
        "        Output:\n",
        "            success (bool): defines whether operation was successfully completed or not.\n",
        "       \"\"\"\n",
        "    if unique:\n",
        "        smiles = list(set(smiles))\n",
        "    else:\n",
        "        smiles = list(smiles)\n",
        "    f = open(filename, 'w')\n",
        "    for mol in smiles:\n",
        "        f.writelines([mol, '\\n'])\n",
        "    f.close()\n",
        "    return f.closed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNiHiVbrYtX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_smi_file(filename, unique=True, add_start_end_tokens=False):\n",
        "    \"\"\"\n",
        "    Reads SMILES from file. File must contain one SMILES string per line\n",
        "    with \\n token in the end of the line.\n",
        "    Args:\n",
        "        filename (str): path to the file\n",
        "        unique (bool): return only unique SMILES\n",
        "    Returns:\n",
        "        smiles (list): list of SMILES strings from specified file.\n",
        "        success (bool): defines whether operation was successfully completed or not.\n",
        "    If 'unique=True' this list contains only unique copies.\n",
        "    \"\"\"\n",
        "    f = open(filename, 'r')\n",
        "    molecules = []\n",
        "    for line in f:\n",
        "        if add_start_end_tokens:\n",
        "            molecules.append('<' + line[:-1] + '>')\n",
        "        else:\n",
        "            molecules.append(line[:-1])\n",
        "    if unique:\n",
        "        molecules = list(set(molecules))\n",
        "    else:\n",
        "        molecules = list(molecules)\n",
        "    f.close()\n",
        "    return molecules, f.closed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYySqz7lYvKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(smiles, tokens=None):\n",
        "    \"\"\"\n",
        "    Returns list of unique tokens, token-2-index dictionary and number of\n",
        "    unique tokens from the list of SMILES\n",
        "    Parameters\n",
        "    ----------\n",
        "        smiles: list\n",
        "            list of SMILES strings to tokenize.\n",
        "        tokens: list, str (default None)\n",
        "            list of unique tokens\n",
        "    Returns\n",
        "    -------\n",
        "        tokens: list\n",
        "            list of unique tokens/SMILES alphabet.\n",
        "        token2idx: dict\n",
        "            dictionary mapping token to its index.\n",
        "        num_tokens: int\n",
        "            number of unique tokens.\n",
        "    \"\"\"\n",
        "    if tokens is None:\n",
        "        tokens = list(set(''.join(smiles)))\n",
        "        tokens = list(np.sort(tokens))\n",
        "        tokens = ''.join(tokens)\n",
        "    token2idx = dict((token, i) for i, token in enumerate(tokens))\n",
        "    num_tokens = len(tokens)\n",
        "    return tokens, token2idx, num_tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHuSouDGYyie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nBYgIEJYw_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation_split(x, y, n_folds=5, split='random', folds=None):\n",
        "    assert(len(x) == len(y))\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    if split not in ['random', 'stratified', 'fixed']:\n",
        "        raise ValueError('Invalid value for argument \\'split\\': '\n",
        "                         'must be either \\'random\\', \\'stratified\\' '\n",
        "                         'or \\'fixed\\'')\n",
        "    if split == 'random':\n",
        "        cv_split = KFold(n_splits=n_folds, shuffle=True)\n",
        "        folds = list(cv_split.split(x, y))\n",
        "    elif split == 'stratified':\n",
        "        cv_split = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
        "        folds = list(cv_split.split(x, y))\n",
        "    elif split == 'fixed' and folds is None:\n",
        "        raise TypeError(\n",
        "            'Invalid type for argument \\'folds\\': found None, but must be list')\n",
        "    cross_val_data = []\n",
        "    cross_val_labels = []\n",
        "    if len(folds) == n_folds:\n",
        "        for fold in folds:\n",
        "            cross_val_data.append(x[fold[1]])\n",
        "            cross_val_labels.append(y[fold[1]])\n",
        "    elif len(folds) == len(x) and np.max(folds) == n_folds:\n",
        "        for f in range(n_folds):\n",
        "            left = np.where(folds == f)[0].min()\n",
        "            right = np.where(folds == f)[0].max()\n",
        "            cross_val_data.append(x[left:right + 1])\n",
        "            cross_val_labels.append(y[left:right + 1])\n",
        "\n",
        "    return cross_val_data, cross_val_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FBT7ziQY1qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_object_property_file(path, delimiter=',', cols_to_read=[0, 1],\n",
        "                              keep_header=False):\n",
        "    f = open(path, 'r')\n",
        "    reader = csv.reader(f, delimiter=delimiter)\n",
        "    data_full = np.array(list(reader))\n",
        "    if keep_header:\n",
        "        start_position = 0\n",
        "    else:\n",
        "        start_position = 1\n",
        "    assert len(data_full) > start_position\n",
        "    data = [[] for _ in range(len(cols_to_read))]\n",
        "    for i in range(len(cols_to_read)):\n",
        "        col = cols_to_read[i]\n",
        "        data[i] = data_full[start_position:, col]\n",
        "    f.close()\n",
        "    if len(cols_to_read) == 1:\n",
        "        data = data[0]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytn5MmqbY7lz",
        "colab_type": "text"
      },
      "source": [
        "### **Data Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIz58LRnZJ9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erBnpdwiZOqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorData(object):\n",
        "    def __init__(self, training_data_path, tokens=None, start_token='<', \n",
        "                 end_token='>', max_len=120, use_cuda=None, **kwargs):\n",
        "\n",
        "        super(GeneratorData, self).__init__()\n",
        "\n",
        "        if 'cols_to_read' not in kwargs:\n",
        "            kwargs['cols_to_read'] = []\n",
        "\n",
        "        data = read_object_property_file(training_data_path,\n",
        "                                                       **kwargs)\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.file = []\n",
        "        for i in range(len(data)):\n",
        "            if len(data[i]) <= max_len:\n",
        "                self.file.append(self.start_token + data[i] + self.end_token) \n",
        "        self.file_len = len(self.file)\n",
        "        self.all_characters, self.char2idx, \\\n",
        "        self.n_characters = tokenize(self.file, tokens)\n",
        "        self.use_cuda = use_cuda\n",
        "        if self.use_cuda is None:\n",
        "            self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "    def load_dictionary(self, tokens, char2idx):\n",
        "        self.all_characters = tokens\n",
        "        self.char2idx = char2idx\n",
        "        self.n_characters = len(tokens)\n",
        "\n",
        "    def random_chunk(self):\n",
        "\n",
        "        index = random.randint(0, self.file_len-1)\n",
        "        return self.file[index]\n",
        "\n",
        "    def char_tensor(self, string):\n",
        "\n",
        "        tensor = torch.zeros(len(string)).long()\n",
        "        for c in range(len(string)):\n",
        "            tensor[c] = self.all_characters.index(string[c])\n",
        "        if self.use_cuda:\n",
        "            return torch.tensor(tensor).cuda()\n",
        "        else:\n",
        "            return torch.tensor(tensor)\n",
        "\n",
        "    def random_training_set(self, smiles_augmentation):\n",
        "        chunk = self.random_chunk()\n",
        "        if smiles_augmentation is not None:\n",
        "            chunk = '<' + smiles_augmentation.randomize_smiles(chunk[1:-1]) + '>'\n",
        "        inp = self.char_tensor(chunk[:-1])\n",
        "        target = self.char_tensor(chunk[1:])\n",
        "        return inp, target\n",
        "\n",
        "    def read_sdf_file(self, path, fields_to_read):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def update_data(self, path):\n",
        "        self.file, success = read_smi_file(path, unique=True)\n",
        "        self.file_len = len(self.file)\n",
        "        assert success\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyiZIGwfZO0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PredictorData(object):\n",
        "    def __init__(self, path, delimiter=',', cols=[0, 1], get_features=None,\n",
        "                 has_label=True, labels_start=1, **kwargs):\n",
        "        super(PredictorData, self).__init__()\n",
        "        data = read_object_property_file(path, delimiter, cols_to_read=cols)\n",
        "        if has_label:\n",
        "            self.objects = np.array(data[:labels_start]).reshape(-1)\n",
        "            self.y = np.array(data[labels_start:], dtype='float32')\n",
        "            self.y = self.y.reshape(-1, len(cols) - labels_start)\n",
        "            if self.y.shape[1] == 1:\n",
        "                self.y = self.y.reshape(-1)\n",
        "        else:\n",
        "            self.objects = np.array(data[:labels_start]).reshape(-1)\n",
        "            self.y = [None]*len(self.object)\n",
        "        assert len(self.objects) == len(self.y)\n",
        "        if get_features is not None:\n",
        "            self.x, processed_indices, invalid_indices = \\\n",
        "                get_features(self.objects, **kwargs)\n",
        "            self.invalid_objects = self.objects[invalid_indices]\n",
        "            self.objects = self.objects[processed_indices]\n",
        "            self.invalid_y = self.y[invalid_indices]\n",
        "            self.y = self.y[processed_indices]\n",
        "        else:\n",
        "            self.x = self.objects\n",
        "            self.invalid_objects = None\n",
        "            self.invalid_y = None\n",
        "        self.binary_y = None\n",
        "\n",
        "    def binarize(self, threshold):\n",
        "        self.binary_y = np.array(self.y >= threshold, dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3rslGElbhNk",
        "colab_type": "text"
      },
      "source": [
        "### **Smiles Enumerator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnOHyVCbsTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import threading"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uzmFW7nbsjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Iterator(object):\n",
        "    \"\"\"Abstract base class for data iterators.\n",
        "    # Arguments\n",
        "        n: Integer, total number of samples in the dataset to loop over.\n",
        "        batch_size: Integer, size of a batch.\n",
        "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
        "        seed: Random seeding for data shuffling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n, batch_size, shuffle, seed):\n",
        "        self.n = n\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_index = 0\n",
        "        self.total_batches_seen = 0\n",
        "        self.lock = threading.Lock()\n",
        "        self.index_generator = self._flow_index(n, batch_size, shuffle, seed)\n",
        "        if n < batch_size:\n",
        "            raise ValueError('Input data length is shorter than batch_size\\nAdjust batch_size')\n",
        "\n",
        "    def reset(self):\n",
        "        self.batch_index = 0\n",
        "\n",
        "    def _flow_index(self, n, batch_size=32, shuffle=False, seed=None):\n",
        "        # Ensure self.batch_index is 0.\n",
        "        self.reset()\n",
        "        while 1:\n",
        "            if seed is not None:\n",
        "                np.random.seed(seed + self.total_batches_seen)\n",
        "            if self.batch_index == 0:\n",
        "                index_array = np.arange(n)\n",
        "                if shuffle:\n",
        "                    index_array = np.random.permutation(n)\n",
        "\n",
        "            current_index = (self.batch_index * batch_size) % n\n",
        "            if n > current_index + batch_size:\n",
        "                current_batch_size = batch_size\n",
        "                self.batch_index += 1\n",
        "            else:\n",
        "                current_batch_size = n - current_index\n",
        "                self.batch_index = 0\n",
        "            self.total_batches_seen += 1\n",
        "            yield (index_array[current_index: current_index + current_batch_size],\n",
        "                   current_index, current_batch_size)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Needed if we want to do something like:\n",
        "        # for x, y in data_gen.flow(...):\n",
        "        return self\n",
        "\n",
        "    def __next__(self, *args, **kwargs):\n",
        "        return self.next(*args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlMnsIfabsnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SmilesIterator(Iterator):\n",
        "    \"\"\"Iterator yielding data from a SMILES array.\n",
        "    # Arguments\n",
        "        x: Numpy array of SMILES input data.\n",
        "        y: Numpy array of targets data.\n",
        "        smiles_data_generator: Instance of `SmilesEnumerator`\n",
        "            to use for random SMILES generation.\n",
        "        batch_size: Integer, size of a batch.\n",
        "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
        "        seed: Random seed for data shuffling.\n",
        "        dtype: dtype to use for returned batch. Set to keras.backend.floatx if using Keras\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, y, smiles_data_generator,\n",
        "                 batch_size=32, shuffle=False, seed=None,\n",
        "                 dtype=np.float32\n",
        "                 ):\n",
        "        if y is not None and len(x) != len(y):\n",
        "            raise ValueError('X (images tensor) and y (labels) '\n",
        "                             'should have the same length. '\n",
        "                             'Found: X.shape = %s, y.shape = %s' %\n",
        "                             (np.asarray(x).shape, np.asarray(y).shape))\n",
        "\n",
        "        self.x = np.asarray(x)\n",
        "\n",
        "        if y is not None:\n",
        "            self.y = np.asarray(y)\n",
        "        else:\n",
        "            self.y = None\n",
        "        self.smiles_data_generator = smiles_data_generator\n",
        "        self.dtype = dtype\n",
        "        super(SmilesIterator, self).__init__(x.shape[0], batch_size, shuffle, seed)\n",
        "\n",
        "    def next(self):\n",
        "        \"\"\"For python 2.x.\n",
        "        # Returns\n",
        "            The next batch.\n",
        "        \"\"\"\n",
        "        # Keeps under lock only the mechanism which advances\n",
        "        # the indexing of each batch.\n",
        "        with self.lock:\n",
        "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
        "        # The transformation of images is not under thread lock\n",
        "        # so it can be done in parallel\n",
        "        batch_x = np.zeros(\n",
        "            tuple([current_batch_size] + [self.smiles_data_generator.pad, self.smiles_data_generator._charlen]),\n",
        "            dtype=self.dtype)\n",
        "        for i, j in enumerate(index_array):\n",
        "            smiles = self.x[j:j + 1]\n",
        "            x = self.smiles_data_generator.transform(smiles)\n",
        "            batch_x[i] = x\n",
        "\n",
        "        if self.y is None:\n",
        "            return batch_x\n",
        "        batch_y = self.y[index_array]\n",
        "        return batch_x, batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTXrGNyDbsgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SmilesEnumerator(object):\n",
        "    \"\"\"SMILES Enumerator, vectorizer and devectorizer\n",
        "    #Arguments\n",
        "        charset: string containing the characters for the vectorization\n",
        "          can also be generated via the .fit() method\n",
        "        pad: Length of the vectorization\n",
        "        leftpad: Add spaces to the left of the SMILES\n",
        "        isomericSmiles: Generate SMILES containing information about stereogenic centers\n",
        "        enum: Enumerate the SMILES during transform\n",
        "        canonical: use canonical SMILES during transform (overrides enum)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, charset='@C)(=cOn1S2/H[N]\\\\', pad=120, leftpad=True, isomericSmiles=True, enum=True,\n",
        "                 canonical=False):\n",
        "        self._charset = None\n",
        "        self.charset = charset\n",
        "        self.pad = pad\n",
        "        self.leftpad = leftpad\n",
        "        self.isomericSmiles = isomericSmiles\n",
        "        self.enumerate = enum\n",
        "        self.canonical = canonical\n",
        "\n",
        "    @property\n",
        "    def charset(self):\n",
        "        return self._charset\n",
        "\n",
        "    @charset.setter\n",
        "    def charset(self, charset):\n",
        "        self._charset = charset\n",
        "        self._charlen = len(charset)\n",
        "        self._char_to_int = dict((c, i) for i, c in enumerate(charset))\n",
        "        self._int_to_char = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "    def fit(self, smiles, extra_chars=[], extra_pad=5):\n",
        "        \"\"\"Performs extraction of the charset and length of a SMILES datasets and sets self.pad and self.charset\n",
        "        #Arguments\n",
        "            smiles: Numpy array or Pandas series containing smiles as strings\n",
        "            extra_chars: List of extra chars to add to the charset (e.g. \"\\\\\\\\\" when \"/\" is present)\n",
        "            extra_pad: Extra padding to add before or after the SMILES vectorization\n",
        "        \"\"\"\n",
        "        charset = set(\"\".join(list(smiles)))\n",
        "        self.charset = \"\".join(charset.union(set(extra_chars)))\n",
        "        self.pad = max([len(smile) for smile in smiles]) + extra_pad\n",
        "\n",
        "    def randomize_smiles(self, smiles):\n",
        "        \"\"\"Perform a randomization of a SMILES string\n",
        "        must be RDKit sanitizable\"\"\"\n",
        "        m = Chem.MolFromSmiles(smiles)\n",
        "        ans = list(range(m.GetNumAtoms()))\n",
        "        np.random.shuffle(ans)\n",
        "        nm = Chem.RenumberAtoms(m, ans)\n",
        "        return Chem.MolToSmiles(nm, canonical=self.canonical, isomericSmiles=self.isomericSmiles)\n",
        "\n",
        "    def transform(self, smiles):\n",
        "        \"\"\"Perform an enumeration (randomization) and vectorization of a Numpy array of smiles strings\n",
        "        #Arguments\n",
        "            smiles: Numpy array or Pandas series containing smiles as strings\n",
        "        \"\"\"\n",
        "        one_hot = np.zeros((smiles.shape[0], self.pad, self._charlen), dtype=np.int8)\n",
        "\n",
        "        for i, ss in enumerate(smiles):\n",
        "            if self.enumerate: ss = self.randomize_smiles(ss)\n",
        "            for j, c in enumerate(ss):\n",
        "                one_hot[i, j, self._char_to_int[c]] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def reverse_transform(self, vect):\n",
        "        \"\"\" Performs a conversion of a vectorized SMILES to a smiles strings\n",
        "        charset must be the same as used for vectorization.\n",
        "        #Arguments\n",
        "            vect: Numpy array of vectorized SMILES.\n",
        "        \"\"\"\n",
        "        smiles = []\n",
        "        for v in vect:\n",
        "            # mask v\n",
        "            v = v[v.sum(axis=1) == 1]\n",
        "            # Find one hot encoded index with argmax, translate to char and join to string\n",
        "            smile = \"\".join(self._int_to_char[i] for i in v.argmax(axis=1))\n",
        "            smiles.append(smile)\n",
        "        return np.array(smiles)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    smiles = np.array([\"CCC(=O)O[C@@]1(CC[NH+](C[C@H]1CC=C)C)c2ccccc2\",\n",
        "                       \"CCC[S@@](=O)c1ccc2c(c1)[nH]/c(=N/C(=O)OC)/[nH]2\"] * 10\n",
        "                      )\n",
        "    # Test canonical SMILES vectorization\n",
        "    sm_en = SmilesEnumerator(canonical=True, enum=False)\n",
        "    sm_en.fit(smiles, extra_chars=[\"\\\\\"])\n",
        "    v = sm_en.transform(smiles)\n",
        "    transformed = sm_en.reverse_transform(v)\n",
        "    if len(set(transformed)) > 2: print(\"Too many different canonical SMILES generated\")\n",
        "\n",
        "    # Test enumeration\n",
        "    sm_en.canonical = False\n",
        "    sm_en.enumerate = True\n",
        "    v2 = sm_en.transform(smiles)\n",
        "    transformed = sm_en.reverse_transform(v2)\n",
        "    if len(set(transformed)) < 3: print(\"Too few enumerated SMILES generated\")\n",
        "\n",
        "    # Reconstruction\n",
        "    reconstructed = sm_en.reverse_transform(v[0:5])\n",
        "    for i, smile in enumerate(reconstructed):\n",
        "        if smile != smiles[i]:\n",
        "            print(\"Error in reconstruction %s %s\" % (smile, smiles[i]))\n",
        "            break\n",
        "\n",
        "    # test Pandas\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.DataFrame(smiles)\n",
        "    v = sm_en.transform(df[0])\n",
        "    if v.shape != (20, 52, 18): print(\"Possible error in pandas use\")\n",
        "\n",
        "    # BUG, when batchsize > x.shape[0], then it only returns x.shape[0]!\n",
        "    # Test batch generation\n",
        "    sm_it = SmilesIterator(smiles, np.array([1, 2] * 10), sm_en, batch_size=10, shuffle=True)\n",
        "    X, y = sm_it.next()\n",
        "    if sum(y == 1) - sum(y == 2) > 1:\n",
        "        print(\"Unbalanced generation of batches\")\n",
        "    if len(X) != 10: print(\"Error in batchsize generation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENGjmE2cbLLW",
        "colab_type": "text"
      },
      "source": [
        "### **StackRNN Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxroIzrCbKT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "from tqdm import trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Z8nXXTcAmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StackAugmentedRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, layer_type='GRU',\n",
        "                 n_layers=1, is_bidirectional=False, has_stack=False,\n",
        "                 stack_width=None, stack_depth=None, use_cuda=None,\n",
        "                 optimizer_instance=torch.optim.Adadelta, lr=0.01):\n",
        "        \"\"\"\n",
        "        Constructor for the StackAugmentedRNN object.\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_size: int\n",
        "            number of characters in the alphabet\n",
        "        hidden_size: int\n",
        "            size of the RNN layer(s)\n",
        "        output_size: int\n",
        "            again number of characters in the alphabet\n",
        "        layer_type: str (default 'GRU')\n",
        "            type of the RNN layer to be used. Could be either 'LSTM' or 'GRU'.\n",
        "        n_layers: int (default 1)\n",
        "            number of RNN layers\n",
        "        is_bidirectional: bool (default False)\n",
        "            parameter specifying if RNN is bidirectional\n",
        "        has_stack: bool (default False)\n",
        "            parameter specifying if augmented memory stack is used\n",
        "        stack_width: int (default None)\n",
        "            if has_stack is True then this parameter defines width of the\n",
        "            augmented stack memory\n",
        "        stack_depth: int (default None)\n",
        "            if has_stack is True then this parameter define depth of the augmented\n",
        "            stack memory. Hint: no need fo stack depth to be larger than the\n",
        "            length of the longest sequence you plan to generate\n",
        "        use_cuda: bool (default None)\n",
        "            parameter specifying if GPU is used for computations. If left\n",
        "            unspecified, GPU will be used if available\n",
        "        optimizer_instance: torch.optim object (default torch.optim.Adadelta)\n",
        "            optimizer to be used for training\n",
        "        lr: float (default 0.01)\n",
        "            learning rate for the optimizer\n",
        "        \"\"\"\n",
        "        super(StackAugmentedRNN, self).__init__()\n",
        "        \n",
        "        if layer_type not in ['GRU', 'LSTM']:\n",
        "            raise InvalidArgumentError('Layer type must be GRU or LSTM')\n",
        "        self.layer_type = layer_type\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "        if self.is_bidirectional:\n",
        "            self.num_dir = 2\n",
        "        else:\n",
        "            self.num_dir = 1\n",
        "        if layer_type == 'LSTM':\n",
        "            self.has_cell = True\n",
        "        else:\n",
        "            self.has_cell = False\n",
        "        self.has_stack = has_stack\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        if self.has_stack:\n",
        "            self.stack_width = stack_width\n",
        "            self.stack_depth = stack_depth\n",
        "\n",
        "        self.use_cuda = use_cuda\n",
        "        if self.use_cuda is None:\n",
        "            self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        if self.has_stack:\n",
        "            self.stack_controls_layer = nn.Linear(in_features=self.hidden_size *\n",
        "                                                              self.num_dir,\n",
        "                                                  out_features=3)\n",
        "\n",
        "            self.stack_input_layer = nn.Linear(in_features=self.hidden_size *\n",
        "                                                           self.num_dir,\n",
        "                                               out_features=self.stack_width)\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        if self.has_stack:\n",
        "            rnn_input_size = hidden_size + stack_width\n",
        "        else:\n",
        "            rnn_input_size = hidden_size\n",
        "        if self.layer_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(rnn_input_size, hidden_size, n_layers,\n",
        "                               bidirectional=self.is_bidirectional)\n",
        "            self.decoder = nn.Linear(hidden_size * self.num_dir, output_size)\n",
        "        elif self.layer_type == 'GRU':\n",
        "            self.rnn = nn.GRU(rnn_input_size, hidden_size, n_layers,\n",
        "                             bidirectional=self.is_bidirectional)\n",
        "            self.decoder = nn.Linear(hidden_size * self.num_dir, output_size)\n",
        "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self = self.cuda()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.lr = lr\n",
        "        self.optimizer_instance = optimizer_instance\n",
        "        self.optimizer = self.optimizer_instance(self.parameters(), lr=lr,\n",
        "                                                 weight_decay=0.00001)\n",
        "  \n",
        "    def load_model(self, path):\n",
        "        \"\"\"\n",
        "        Loads pretrained parameters from the checkpoint into the model.\n",
        "        Parameters\n",
        "        ----------\n",
        "        path: str\n",
        "            path to the checkpoint file model will be loaded from.\n",
        "        \"\"\"\n",
        "        weights = torch.load(path)\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"\n",
        "        Saves model parameters into the checkpoint file.\n",
        "        Parameters\n",
        "        ----------\n",
        "        path: str\n",
        "            path to the checkpoint file model will be saved to.\n",
        "        \"\"\"\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def change_lr(self, new_lr):\n",
        "        \"\"\"\n",
        "        Updates learning rate of the optimizer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        new_lr: float\n",
        "            new learning rate value\n",
        "        \"\"\"\n",
        "        self.optimizer = self.optimizer_instance(self.parameters(), lr=new_lr)\n",
        "        self.lr = new_lr\n",
        "\n",
        "    def forward(self, inp, hidden, stack):\n",
        "        \"\"\"\n",
        "        Forward step of the model. Generates probability of the next character\n",
        "        given the prefix.\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp: torch.tensor\n",
        "            input tensor that contains prefix string indices\n",
        "        hidden: torch.tensor or tuple(torch.tensor, torch.tensor)\n",
        "            previous hidden state of the model. If layer_type is 'LSTM',\n",
        "            then hidden is a tuple of hidden state and cell state, otherwise\n",
        "            hidden is torch.tensor\n",
        "        stack: torch.tensor\n",
        "            previous state of the augmented memory stack\n",
        "        Returns\n",
        "        -------\n",
        "        output: torch.tensor\n",
        "            tensor with non-normalized probabilities of the next character\n",
        "        next_hidden: torch.tensor or tuple(torch.tensor, torch.tensor)\n",
        "            next hidden state of the model. If layer_type is 'LSTM',\n",
        "            then next_hidden is a tuple of hidden state and cell state,\n",
        "            otherwise next_hidden is torch.tensor\n",
        "        next_stack: torch.tensor\n",
        "            next state of the augmented memory stack\n",
        "        \"\"\"\n",
        "        inp = self.encoder(inp.view(1, -1))\n",
        "        if self.has_stack:\n",
        "            if self.has_cell:\n",
        "                hidden_ = hidden[0]\n",
        "            else:\n",
        "                hidden_ = hidden\n",
        "            if self.is_bidirectional:\n",
        "                hidden_2_stack = torch.cat((hidden_[0], hidden_[1]), dim=1)\n",
        "            else:\n",
        "                hidden_2_stack = hidden_.squeeze(0)\n",
        "            stack_controls = self.stack_controls_layer(hidden_2_stack)\n",
        "            stack_controls = F.softmax(stack_controls, dim=1)\n",
        "            stack_input = self.stack_input_layer(hidden_2_stack.unsqueeze(0))\n",
        "            stack_input = torch.tanh(stack_input)\n",
        "            stack = self.stack_augmentation(stack_input.permute(1, 0, 2),\n",
        "                                            stack, stack_controls)\n",
        "            stack_top = stack[:, 0, :].unsqueeze(0)\n",
        "            inp = torch.cat((inp, stack_top), dim=2)\n",
        "        output, next_hidden = self.rnn(inp.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, next_hidden, stack\n",
        "\n",
        "    def stack_augmentation(self, input_val, prev_stack, controls):\n",
        "        \"\"\"\n",
        "        Augmentation of the tensor into the stack. For more details see\n",
        "        https://arxiv.org/abs/1503.01007\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_val: torch.tensor\n",
        "            tensor to be added to stack\n",
        "        prev_stack: torch.tensor\n",
        "            previous stack state\n",
        "        controls: torch.tensor\n",
        "            predicted probabilities for each operation in the stack, i.e\n",
        "            PUSH, POP and NO_OP. Again, see https://arxiv.org/abs/1503.01007\n",
        "        Returns\n",
        "        -------\n",
        "        new_stack: torch.tensor\n",
        "            new stack state\n",
        "        \"\"\"\n",
        "        batch_size = prev_stack.size(0)\n",
        "\n",
        "        controls = controls.view(-1, 3, 1, 1)\n",
        "        zeros_at_the_bottom = torch.zeros(batch_size, 1, self.stack_width)\n",
        "        if self.use_cuda:\n",
        "            zeros_at_the_bottom = Variable(zeros_at_the_bottom.cuda())\n",
        "        else:\n",
        "            zeros_at_the_bottom = Variable(zeros_at_the_bottom)\n",
        "        a_push, a_pop, a_no_op = controls[:, 0], controls[:, 1], controls[:, 2]\n",
        "        stack_down = torch.cat((prev_stack[:, 1:], zeros_at_the_bottom), dim=1)\n",
        "        stack_up = torch.cat((input_val, prev_stack[:, :-1]), dim=1)\n",
        "        new_stack = a_no_op * prev_stack + a_push * stack_up + a_pop * stack_down\n",
        "        return new_stack\n",
        "\n",
        "    def init_hidden(self):\n",
        "        \"\"\"\n",
        "        Initialization of the hidden state of RNN.\n",
        "        Returns\n",
        "        -------\n",
        "        hidden: torch.tensor\n",
        "            tensor filled with zeros of an appropriate size (taking into\n",
        "            account number of RNN layers and directions)\n",
        "        \"\"\"\n",
        "        if self.use_cuda:\n",
        "            return Variable(torch.zeros(self.n_layers * self.num_dir, 1,\n",
        "                                        self.hidden_size).cuda())\n",
        "        else:\n",
        "            return Variable(torch.zeros(self.n_layers * self.num_dir, 1,\n",
        "                                        self.hidden_size))\n",
        "\n",
        "    def init_cell(self):\n",
        "        \"\"\"\n",
        "        Initialization of the cell state of LSTM. Only used when layers_type is\n",
        "        'LSTM'\n",
        "        Returns\n",
        "        -------\n",
        "        cell: torch.tensor\n",
        "            tensor filled with zeros of an appropriate size (taking into\n",
        "            account number of RNN layers and directions)\n",
        "        \"\"\"\n",
        "        if self.use_cuda:\n",
        "            return Variable(torch.zeros(self.n_layers * self.num_dir, 1,\n",
        "                                        self.hidden_size).cuda())\n",
        "        else:\n",
        "            return Variable(torch.zeros(self.n_layers * self.num_dir, 1,\n",
        "                                        self.hidden_size))\n",
        "\n",
        "    def init_stack(self):\n",
        "        \"\"\"\n",
        "        Initialization of the stack state. Only used when has_stack is True\n",
        "        Returns\n",
        "        -------\n",
        "        stack: torch.tensor\n",
        "            tensor filled with zeros\n",
        "        \"\"\"\n",
        "        result = torch.zeros(1, self.stack_depth, self.stack_width)\n",
        "        if self.use_cuda:\n",
        "            return Variable(result.cuda())\n",
        "        else:\n",
        "            return Variable(result)\n",
        "\n",
        "    def train_step(self, inp, target):\n",
        "        \"\"\"\n",
        "        One train step, i.e. forward-backward and parameters update, for\n",
        "        a single training example.\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp: torch.tensor\n",
        "            tokenized training string from position 0 to position (seq_len - 1)\n",
        "        target:\n",
        "            tokenized training string from position 1 to position seq_len\n",
        "        Returns\n",
        "        -------\n",
        "        loss: float\n",
        "            mean value of the loss function (averaged through the sequence\n",
        "            length)\n",
        "        \"\"\"\n",
        "        hidden = self.init_hidden()\n",
        "        if self.has_cell:\n",
        "            cell = self.init_cell()\n",
        "            hidden = (hidden, cell)\n",
        "        if self.has_stack:\n",
        "            stack = self.init_stack()\n",
        "        else:\n",
        "            stack = None\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        for c in range(len(inp)):\n",
        "            output, hidden, stack = self(inp[c], hidden, stack)\n",
        "            loss += self.criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item() / len(inp)\n",
        "    \n",
        "    def evaluate(self, data, prime_str='<', end_token='>', predict_len=100):\n",
        "        \"\"\"\n",
        "        Generates new string from the model distribution.\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: object of type GeneratorData\n",
        "            stores information about the generator data format such alphabet, etc\n",
        "        prime_str: str (default '<')\n",
        "            prime string that will be used as prefix. Deafult value is just the\n",
        "            START_TOKEN\n",
        "        end_token: str (default '>')\n",
        "            when end_token is sampled from the model distribution,\n",
        "            the generation of a new example is finished\n",
        "        predict_len: int (default 100)\n",
        "            maximum length of the string to be generated. If the end_token is\n",
        "            not sampled, the generation will be aborted when the length of the\n",
        "            generated sequence is equal to predict_len\n",
        "        Returns\n",
        "        -------\n",
        "        new_sample: str\n",
        "            Newly generated sample from the model distribution.\n",
        "        \"\"\"\n",
        "        hidden = self.init_hidden()\n",
        "        if self.has_cell:\n",
        "            cell = self.init_cell()\n",
        "            hidden = (hidden, cell)\n",
        "        if self.has_stack:\n",
        "            stack = self.init_stack()\n",
        "        else:\n",
        "            stack = None\n",
        "        prime_input = data.char_tensor(prime_str)\n",
        "        new_sample = prime_str\n",
        "\n",
        "        # Use priming string to \"build up\" hidden state\n",
        "        for p in range(len(prime_str)-1):\n",
        "            _, hidden, stack = self.forward(prime_input[p], hidden, stack)\n",
        "        inp = prime_input[-1]\n",
        "\n",
        "        for p in range(predict_len):\n",
        "            output, hidden, stack = self.forward(inp, hidden, stack)\n",
        "\n",
        "            # Sample from the network as a multinomial distribution\n",
        "            probs = torch.softmax(output, dim=1)\n",
        "            top_i = torch.multinomial(probs.view(-1), 1)[0].cpu().numpy()\n",
        "\n",
        "            # Add predicted character to string and use as next input\n",
        "            predicted_char = data.all_characters[top_i]\n",
        "            new_sample += predicted_char\n",
        "            inp = data.char_tensor(predicted_char)\n",
        "            if predicted_char == end_token:\n",
        "                break\n",
        "\n",
        "        return new_sample\n",
        "\n",
        "    def fit(self, data, n_iterations, all_losses=[], print_every=100,\n",
        "            plot_every=10, augment=False):\n",
        "        \"\"\"\n",
        "        This methods fits the parameters of the model. Training is performed to\n",
        "        minimize the cross-entropy loss when predicting the next character\n",
        "        given the prefix.\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: object of type GeneratorData\n",
        "            stores information about the generator data format such alphabet, etc\n",
        "        n_iterations: int\n",
        "            how many iterations of training will be performed\n",
        "        all_losses: list (default [])\n",
        "            list to store the values of the loss function\n",
        "        print_every: int (default 100)\n",
        "            feedback will be printed to std_out once every print_every\n",
        "            iterations of training\n",
        "        plot_every: int (default 10)\n",
        "            value of the loss function will be appended to all_losses once every\n",
        "            plot_every iterations of training\n",
        "        augment: bool (default False)\n",
        "            parameter specifying if SMILES enumeration will be used. For mode\n",
        "            details on SMILES enumeration see https://arxiv.org/abs/1703.07076\n",
        "        Returns\n",
        "        -------\n",
        "        all_losses: list\n",
        "            list that stores the values of the loss function (learning curve)\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "        loss_avg = 0\n",
        "\n",
        "        if augment:\n",
        "            smiles_augmentation = SmilesEnumerator()\n",
        "        else:\n",
        "            smiles_augmentation = None\n",
        "\n",
        "        for epoch in trange(1, n_iterations + 1, desc='Training in progress...'):\n",
        "            inp, target = data.random_training_set(smiles_augmentation)\n",
        "            loss = self.train_step(inp, target)\n",
        "            loss_avg += loss\n",
        "\n",
        "            if epoch % print_every == 0:\n",
        "                print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch,\n",
        "                                               epoch / n_iterations * 100, loss)\n",
        "                      )\n",
        "                print(self.evaluate(data=data, prime_str = '<',\n",
        "                                    predict_len=100), '\\n')\n",
        "\n",
        "            if epoch % plot_every == 0:\n",
        "                all_losses.append(loss_avg / plot_every)\n",
        "                loss_avg = 0\n",
        "        return all_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBwWzIjNhYZ8",
        "colab_type": "text"
      },
      "source": [
        "### **Predictor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcV4gmndhbZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpV2jo59hcmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VanillaQSAR(object):\n",
        "    def __init__(self, model_instance=None, model_params=None,\n",
        "                 model_type='classifier', ensemble_size=5, normalization=False):\n",
        "        super(VanillaQSAR, self).__init__()\n",
        "        self.model_instance = model_instance\n",
        "        self.model_params = model_params\n",
        "        self.ensemble_size = ensemble_size\n",
        "        self.model = []\n",
        "        self.normalization = normalization\n",
        "        if model_type not in ['classifier', 'regressor']:\n",
        "            raise InvalidArgumentError(\"model type must be either\"\n",
        "                                       \"classifier or regressor\")\n",
        "        self.model_type = model_type\n",
        "        if isinstance(self.model_instance, list):\n",
        "            assert(len(self.model_instance) == self.ensemble_size)\n",
        "            assert(isinstance(self.model_params, list))\n",
        "            assert(len(self.model_params) == self.ensemble_size)\n",
        "            for i in range(self.ensemble_size):\n",
        "                self.model.append(self.model_instance[i](**model_params[i]))\n",
        "        else:\n",
        "            for _ in range(self.ensemble_size):\n",
        "                self.model.append(self.model_instance(**model_params))\n",
        "        if self.normalization:\n",
        "            self.desc_mean = [0]*self.ensemble_size\n",
        "        self.metrics_type = None\n",
        "\n",
        "    def fit_model(self, data, cv_split='stratified'):\n",
        "        eval_metrics = []\n",
        "        x = data.x\n",
        "        if self.model_type == 'classifier' and data.binary_y is not None:\n",
        "            y = data.binary_y\n",
        "        else:\n",
        "            y = data.y\n",
        "        cross_val_data, cross_val_labels = cross_validation_split(x=x, y=y,\n",
        "                                                                  split=cv_split,\n",
        "                                                                  n_folds=self.ensemble_size)\n",
        "        for i in range(self.ensemble_size):\n",
        "            train_x = np.concatenate(cross_val_data[:i] +\n",
        "                                     cross_val_data[(i + 1):])\n",
        "            test_x = cross_val_data[i]\n",
        "            train_y = np.concatenate(cross_val_labels[:i] +\n",
        "                                     cross_val_labels[(i + 1):])\n",
        "            test_y = cross_val_labels[i]\n",
        "            if self.normalization:\n",
        "                train_x, desc_mean = normalize_desc(train_x)\n",
        "                self.desc_mean[i] = desc_mean\n",
        "                test_x, _ = normalize_desc(test_x, desc_mean)\n",
        "            self.model[i].fit(train_x, train_y.ravel())\n",
        "            predicted = self.model[i].predict(test_x)\n",
        "            if self.model_type == 'classifier':\n",
        "                eval_metrics.append(metrics.f1_score(test_y, predicted))\n",
        "                self.metrics_type = 'F1 score'\n",
        "            elif self.model_type == 'regressor':\n",
        "                r2 = metrics.r2_score(test_y, predicted)\n",
        "                eval_metrics.append(r2)\n",
        "                self.metrics_type = 'R^2 score'\n",
        "            else:\n",
        "                raise RuntimeError()\n",
        "        return eval_metrics, self.metrics_type\n",
        "\n",
        "    def load_model(self, path):\n",
        "        # TODO: add iterable path object instead of static path \n",
        "        self.model = []\n",
        "        for i in range(self.ensemble_size):\n",
        "            m = joblib.load(path + str(i) + '.pkl')\n",
        "            self.model.append(m)\n",
        "        if self.normalization:\n",
        "            arr = np.load(path + 'desc_mean.npy')\n",
        "            self.desc_mean = arr\n",
        "\n",
        "    def save_model(self, path):\n",
        "        assert self.ensemble_size == len(self.model)\n",
        "        for i in range(self.ensemble_size):\n",
        "            joblib.dump(self.model[i], path + str(i) + '.pkl')\n",
        "        if self.normalization:\n",
        "            np.save(path + 'desc_mean.npy', self.desc_mean)\n",
        "\n",
        "    def predict(self, objects=None, average=True, get_features=None,\n",
        "                **kwargs):\n",
        "        objects = np.array(objects)\n",
        "        invalid_objects = []\n",
        "        processed_objects = []\n",
        "        if get_features is not None:\n",
        "            x, processed_indices, invalid_indices = get_features(objects,\n",
        "                                                                 **kwargs)\n",
        "            processed_objects = objects[processed_indices]\n",
        "            invalid_objects = objects[invalid_indices]\n",
        "        else:\n",
        "            x = objects\n",
        "        if len(x) == 0:\n",
        "            processed_objects = []\n",
        "            prediction = []\n",
        "            invalid_objects = objects\n",
        "        else:\n",
        "            prediction = []\n",
        "            for i in range(self.ensemble_size):\n",
        "                m = self.model[i]\n",
        "                if self.normalization:\n",
        "                    x, _ = normalize_desc(x, self.desc_mean[i])\n",
        "                prediction.append(m.predict(x))\n",
        "            prediction = np.array(prediction)\n",
        "            if average:\n",
        "                prediction = prediction.mean(axis=0)\n",
        "        return processed_objects, prediction, invalid_objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZxClZHoh84R",
        "colab_type": "text"
      },
      "source": [
        "### **Reinforcement.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT28fraYiCe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from rdkit import Chem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlyCXt7piCq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reinforcement(object):\n",
        "    def __init__(self, generator, predictor, get_reward):\n",
        "        \"\"\"\n",
        "        Constructor for the Reinforcement object.\n",
        "        Parameters\n",
        "        ----------\n",
        "        generator: object of type StackAugmentedRNN\n",
        "            generative model that produces string of characters (trajectories)\n",
        "        predictor: object of any predictive model type\n",
        "            predictor accepts a trajectory and returns a numerical\n",
        "            prediction of desired property for the given trajectory\n",
        "        get_reward: function\n",
        "            custom reward function that accepts a trajectory, predictor and\n",
        "            any number of positional arguments and returns a single value of\n",
        "            the reward for the given trajectory\n",
        "            Example:\n",
        "            reward = get_reward(trajectory=my_traj, predictor=my_predictor,\n",
        "                                custom_parameter=0.97)\n",
        "        Returns\n",
        "        -------\n",
        "        object of type Reinforcement used for biasing the properties estimated\n",
        "        by the predictor of trajectories produced by the generator to maximize\n",
        "        the custom reward function get_reward.\n",
        "        \"\"\"\n",
        "\n",
        "        super(Reinforcement, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.predictor = predictor\n",
        "        self.get_reward = get_reward\n",
        "\n",
        "    def policy_gradient(self, data, n_batch=10, gamma=0.97,\n",
        "                        std_smiles=False, grad_clipping=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Implementation of the policy gradient algorithm.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data: object of type GeneratorData\n",
        "            stores information about the generator data format such alphabet, etc\n",
        "        n_batch: int (default 10)\n",
        "            number of trajectories to sample per batch. When training on GPU\n",
        "            setting this parameter to to some relatively big numbers can result\n",
        "            in out of memory error. If you encountered such an error, reduce\n",
        "            n_batch.\n",
        "        gamma: float (default 0.97)\n",
        "            factor by which rewards will be discounted within one trajectory.\n",
        "            Usually this number will be somewhat close to 1.0.\n",
        "        std_smiles: bool (default False)\n",
        "            boolean parameter defining whether the generated trajectories will\n",
        "            be converted to standardized SMILES before running policy gradient.\n",
        "            Leave this parameter to the default value if your trajectories are\n",
        "            not SMILES.\n",
        "        grad_clipping: float (default None)\n",
        "            value of the maximum norm of the gradients. If not specified,\n",
        "            the gradients will not be clipped.\n",
        "        kwargs: any number of other positional arguments required by the\n",
        "            get_reward function.\n",
        "        Returns\n",
        "        -------\n",
        "        total_reward: float\n",
        "            value of the reward averaged through n_batch sampled trajectories\n",
        "        rl_loss: float\n",
        "            value for the policy_gradient loss averaged through n_batch sampled\n",
        "            trajectories\n",
        "        \"\"\"\n",
        "        rl_loss = 0\n",
        "        self.generator.optimizer.zero_grad()\n",
        "        total_reward = 0\n",
        "        \n",
        "        for _ in range(n_batch):\n",
        "\n",
        "            # Sampling new trajectory\n",
        "            reward = 0\n",
        "            trajectory = '<>'\n",
        "            while reward == 0:\n",
        "                trajectory = self.generator.evaluate(data)\n",
        "                if std_smiles:\n",
        "                    try:\n",
        "                        mol = Chem.MolFromSmiles(trajectory[1:-1])\n",
        "                        trajectory = '<' + Chem.MolToSmiles(mol) + '>'\n",
        "                        reward = self.get_reward(trajectory[1:-1], \n",
        "                                                 self.predictor, \n",
        "                                                 **kwargs)\n",
        "                    except:\n",
        "                        reward = 0\n",
        "                else:\n",
        "                    reward = self.get_reward(trajectory[1:-1],\n",
        "                                             self.predictor, \n",
        "                                             **kwargs)\n",
        "\n",
        "            # Converting string of characters into tensor\n",
        "            trajectory_input = data.char_tensor(trajectory)\n",
        "            discounted_reward = reward\n",
        "            total_reward += reward\n",
        "\n",
        "            # Initializing the generator's hidden state\n",
        "            hidden = self.generator.init_hidden()\n",
        "            if self.generator.has_cell:\n",
        "                cell = self.generator.init_cell()\n",
        "                hidden = (hidden, cell)\n",
        "            if self.generator.has_stack:\n",
        "                stack = self.generator.init_stack()\n",
        "            else:\n",
        "                stack = None\n",
        "\n",
        "            # \"Following\" the trajectory and accumulating the loss\n",
        "            for p in range(len(trajectory)-1):\n",
        "                output, hidden, stack = self.generator(trajectory_input[p], \n",
        "                                                       hidden, \n",
        "                                                       stack)\n",
        "                log_probs = F.log_softmax(output, dim=1)\n",
        "                top_i = trajectory_input[p+1]\n",
        "                rl_loss -= (log_probs[0, top_i]*discounted_reward)\n",
        "                discounted_reward = discounted_reward * gamma\n",
        "\n",
        "        # Doing backward pass and parameters update\n",
        "        rl_loss = rl_loss / n_batch\n",
        "        total_reward = total_reward / n_batch\n",
        "        rl_loss.backward()\n",
        "        if grad_clipping is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), \n",
        "                                           grad_clipping)\n",
        "\n",
        "        self.generator.optimizer.step()\n",
        "        \n",
        "        return total_reward, rl_loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pz3z9t8cKo5",
        "colab_type": "text"
      },
      "source": [
        "### **DEMO Maximize JAK2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq_8ZqmocRtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3oSREfCcUJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CDwPiLfcUTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append('./release/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42hm_7s4cUeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sydzne8CcUij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1DAO4N0cUQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import pickle\n",
        "from rdkit import Chem, DataStructs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6OuTXf3cUOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ZJsc3Icj1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfr-BlyceQFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data for generative model to train or VAE model \n",
        "gen_data_path = '/content/drive/My Drive/data/JAK2/chembl_22_clean_1576904_sorted_std_final.smi'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJx8QKQveIdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = ['<', '>', '#', '%', ')', '(', '+', '-', '/', '.', '1', '0', '3', '2', '5', '4', '7',\n",
        "          '6', '9', '8', '=', 'A', '@', 'C', 'B', 'F', 'I', 'H', 'O', 'N', 'P', 'S', '[', ']',\n",
        "          '\\\\', 'c', 'e', 'i', 'l', 'o', 'n', 'p', 's', 'r', '\\n']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c60msUebeNol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use data generator\n",
        "gen_data = GeneratorData(training_data_path=gen_data_path, delimiter='\\t', \n",
        "                         cols_to_read=[0], keep_header=True, tokens=tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhKEIgiBebSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#---------------------------distribution of molecules pre-training----------------------------\n",
        "#-------------------for the stack memory RNN model, we don't have this for VAE :/-----------------------\n",
        "def plot_hist(prediction, n_to_generate):\n",
        "    print(\"Mean value of predictions:\", prediction.mean())\n",
        "    print(\"Proportion of valid SMILES:\", len(prediction)/n_to_generate)\n",
        "    ax = sns.kdeplot(prediction, shade=True)\n",
        "    ax.set(xlabel='Predicted pIC50', \n",
        "           title='Distribution of predicted pIC50 for generated molecules')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgkBN6utepVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#estimate update function is used for training - loops through stuff, if reward is greater than expected it back progs a certain way basically \n",
        "def estimate_and_update(generator, predictor, n_to_generate, **kwargs):\n",
        "    generated = []\n",
        "    pbar = tqdm(range(n_to_generate))\n",
        "    for i in pbar:\n",
        "        pbar.set_description(\"Generating molecules...\")\n",
        "        generated.append(generator.evaluate(gen_data, predict_len=120)[1:-1])\n",
        "    #canoical smiles - SMILES string has to be in this format \n",
        "    sanitized = canonical_smiles(generated, sanitize=False, throw_warning=False)[:-1]\n",
        "    unique_smiles = list(np.unique(sanitized))[1:]\n",
        "    smiles, prediction, nan_smiles = predictor.predict(unique_smiles, get_features=get_fp)  \n",
        "                                                       \n",
        "    plot_hist(prediction, n_to_generate)\n",
        "        \n",
        "    return smiles, prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpN-TE8nerY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 1500\n",
        "stack_width = 1500\n",
        "stack_depth = 200\n",
        "layer_type = 'GRU'\n",
        "lr = 0.001\n",
        "optimizer_instance = torch.optim.Adadelta\n",
        "\n",
        "#------------------stack memory RNN model for generative model \n",
        "my_generator = StackAugmentedRNN(input_size=gen_data.n_characters, hidden_size=hidden_size,\n",
        "                                 output_size=gen_data.n_characters, layer_type=layer_type,\n",
        "                                 n_layers=1, is_bidirectional=False, has_stack=True,\n",
        "                                 stack_width=stack_width, stack_depth=stack_depth, \n",
        "                                 use_cuda=use_cuda, \n",
        "                                 optimizer_instance=optimizer_instance, lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlpYsWeKeuZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/drive/My Drive/checkpoint (1)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elNUH6p2e7--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_generator.load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwKkRxoicXeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall tensorflow #runtime = none \n",
        "!pip uninstall tensorflow-gpu\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "!pip show tensorflow \n",
        "#tensorflow must be upgraded to suppot tensorflow-contrib & eager execution \n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade Tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBnBY4VBfTkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade Tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYtcHUekMvX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeUjSnHGMKJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To determine which version you're using:\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJLCRSs-dwtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now training model on the new 1 million molecules SMILES string dataset from chembl\n",
        "data_path = \"/content/drive/My Drive/data/VAE model/zinc/250k_smiles.csv\"\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#read file \n",
        "df = pd.read_csv(data_path, sep='delimiter', header=None)\n",
        "\n",
        "data_smiles = df.to_numpy()\n",
        "\n",
        "data_array = []\n",
        "\n",
        "#iterate through data_smiles and add smiles to data_array\n",
        "for i in range(0, len(data_smiles)):\n",
        "  data_array.append(data_smiles[i][0])\n",
        "\n",
        "#covert data_array (previously data_smiles) to numpy array \n",
        "import numpy as np\n",
        "train_smiles = np.asarray(data_array) \n",
        "print(type(train_smiles))\n",
        "\n",
        "#print 1st SMILES string just to see SMILES string format \n",
        "print(train_smiles[0])\n",
        "\n",
        "tokens = set()\n",
        "for s in train_smiles:\n",
        "  tokens = tokens.union(set(s))\n",
        "tokens = sorted(list(tokens))\n",
        "max_length = max(len(s) for s in train_smiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1z-6kZYTTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------------VAE MODEL GENERATIVE PLATFORM RECALL---------------------\n",
        "import tensorflow as tf\n",
        "import deepchem as dc\n",
        "\n",
        "#the ecoder = CNN and the decoder = GRU\n",
        "from deepchem.models.tensorgraph.optimizers import Adam, ExponentialDecay\n",
        "from deepchem.models.tensorgraph.models.seqtoseq import AspuruGuzikAutoEncoder\n",
        "#declaring model which is ASpuruGuzikAutoEncoder, pre-trained model in deepchem library \n",
        "model = AspuruGuzikAutoEncoder(tokens, max_length, model_dir='vae')\n",
        "\n",
        "batches_per_epoch = len(train_smiles)/model.batch_size\n",
        "learning_rate = ExponentialDecay(0.001, 0.95, batches_per_epoch)\n",
        "model.set_optimizer(Adam(learning_rate=learning_rate))\n",
        "\n",
        "#Generating molecules \n",
        "def generate_sequences(epochs): \n",
        "  for i in range(epochs):\n",
        "    for s in train_smiles: \n",
        "      yield (s, s)\n",
        "\n",
        "#deepchem has its own fit model variation\n",
        "model.fit_sequences(generate_sequences(10))\n",
        "\n",
        "# --- Previously used VAE generator ---\n",
        "#the ecoder = CNN and the decoder = GRU\n",
        "from deepchem.models.tensorgraph.optimizers import Adam, ExponentialDecay\n",
        "from deepchem.models.tensorgraph.models.seqtoseq import AspuruGuzikAutoEncoder\n",
        "#declaring model which is ASpuruGuzikAutoEncoder, pre-trained model in deepchem library \n",
        "gen_model = AspuruGuzikAutoEncoder(tokens, max_length, model_dir='vae')\n",
        "\n",
        "batches_per_epoch = len(train_smiles)/gen_model.batch_size\n",
        "learning_rate = ExponentialDecay(0.001, 0.95, batches_per_epoch)\n",
        "gen_model.set_optimizer(Adam(learning_rate=learning_rate))\n",
        "\n",
        "#restoring VAE model checkpoint file \n",
        "gen_model.restore(checkpoint = \"/content/drive/My Drive/checkpoint (1)\")\n",
        "#gen_model.restore(checkpoint = model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFmkDhjFfwZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mordred\n",
        "from mordred import Calculator, descriptors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCpFUbXkgxYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calc = Calculator(descriptors, ignore_3D=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syckk45rgymY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_data = PredictorData(path='/content/drive/My Drive/data/JAK2/jak2_data.csv', get_features=get_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwh42YxcgyyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor as RFR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIeeNd5Lgy9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_instance = RFR\n",
        "model_params = {'n_estimators': 250, 'n_jobs': 10}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrVtfpBbgzN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_predictor = VanillaQSAR(model_instance=model_instance,\n",
        "                           model_params=model_params,\n",
        "                           model_type='regressor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF5ag9oggzGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_predictor.fit_model(pred_data, cv_split='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NegySOi_hu-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smiles_unbiased, prediction_unbiased = estimate_and_update(my_generator,\n",
        "                                                           my_predictor,\n",
        "                                                           n_to_generate=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5lMz5oYhvWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_to_generate = 200\n",
        "n_policy_replay = 10\n",
        "n_policy = 15\n",
        "n_iterations = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_RANBAhvgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_moving_average(previous_values, new_value, ma_window_size=10):\n",
        "    value_ma = np.sum(previous_values[-(ma_window_size-1):]) + new_value\n",
        "    value_ma = value_ma/(len(previous_values[-(ma_window_size-1):]) + 1)\n",
        "    return value_ma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQIDitH7iUhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_reward_max(smiles, predictor, invalid_reward=0.0, get_features=get_fp):\n",
        "    mol, prop, nan_smiles = predictor.predict([smiles], get_features=get_features)\n",
        "    if len(nan_smiles) == 1:\n",
        "        return invalid_reward\n",
        "    return np.exp(prop[0]/3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MLt7hKqiWSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(0, 12)\n",
        "y = np.exp(x/3)\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('pIC50 value')\n",
        "plt.ylabel('Reward value')\n",
        "plt.title('Reward function for JAK2 activity maximization')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXsaRs7niZGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"defining RL model which takes in the generative model, property \n",
        "prediction model & custom reward function\"\"\"\n",
        "RL_max = Reinforcement(gen_model, my_predictor, get_reward_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvH6uVI9iaRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards_max = []\n",
        "rl_losses_max = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAw8jvwXiaaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training RL model \n",
        "for i in range(n_iterations):\n",
        "    for j in trange(n_policy, desc='Policy gradient...'):\n",
        "        cur_reward, cur_loss = RL_max.policy_gradient(gen_data, \n",
        "                                                      get_features=get_fp)\n",
        "        rewards_max.append(simple_moving_average(rewards_max, cur_reward)) \n",
        "        rl_losses_max.append(simple_moving_average(rl_losses_max, cur_loss))\n",
        "    \n",
        "    plt.plot(rewards_max)\n",
        "    plt.xlabel('Training iteration')\n",
        "    plt.ylabel('Average reward')\n",
        "    plt.show()\n",
        "    plt.plot(rl_losses_max)\n",
        "    plt.xlabel('Training iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "  \n",
        "    #Generating new molecules + predict pic50 \n",
        "    smiles_cur, prediction_cur = estimate_and_update(RL_max.generator, \n",
        "                                                     my_predictor, \n",
        "                                                     n_to_generate,\n",
        "                                                     get_features=get_fp)\n",
        "    print('Sample trajectories:')\n",
        "    for sm in smiles_cur[:5]:\n",
        "        print(sm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQJHSDQXiah2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing RL model\n",
        "smiles_biased_max, prediction_biased_max = estimate_and_update(RL_max.generator, \n",
        "                                                           my_predictor,\n",
        "                                                           n_to_generate=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztepvz0JiatR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.kdeplot(prediction_biased_max,label='Maximized', shade=True, color='red')\n",
        "sns.kdeplot(prediction_unbiased, label='Unbiased', shade=True, color='grey')\n",
        "plt.xlabel('pIC50 values')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}